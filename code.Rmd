---
output:
 pdf_document: 
    fig_caption: yes
    keep_tex: yes
    number_sections: no
    # latex_engine: xelatex
   # header_includes: \usepackage{float}

---

```{r setup, include=FALSE}
library(plm)
library(RCurl)
library(dplyr)
library(plyr)
# library(emibase)
library(plotrix)
# library(mgcv)
library(reshape2)
library(readxl)
library(knitr)
library(rworldmap)
library(cshapes)
library(countrycode)
library(texreg)
library(ggplot2)
library(directlabels)
# library(splines)
library(MASS)
library(TTR)
library(rsdmx)
library(httr)
library(splines)
source('functions.R')
```

\vskip 20pt
\centerline{\bf \large VILNIUS UNIVERSITY}
\bigskip
\centerline{\large \textbf{FACULTY OF MATHEMATICS AND INFORMATICS}}
\vskip 120pt
\centerline{\bf \Large \textbf{BACHELOR THESIS}}
\vskip 50pt
\begin{center}
{\bf \LARGE Tourism model in OECD countries}

\vspace{4mm}

\end{center}
\vskip 120pt
\centerline{\Large Eligijus Bujokas}
\vskip 120pt
\centerline{\large \textbf{VILNIUS (2016.12.07)}}

\newpage

\tableofcontents

\newpage

```{r constants, include=FALSE}
years.to.survey <- 1970:2015
path <- "data/"
# regions <- read.csv(file="input/regions.csv", stringsAsFactors = F)
createdir("plots")
# createdir("input")
createdir("data")
createdir("output")
# devtools::install_url("http://cran.r-project.org/src/contrib/rprojroot_1.2.tar.gz") ## MAY RESOLVE TROUBLESHOOTS
knitr::opts_chunk$set(fig.pos = 'h')
```

```{r data download and reading it, include=FALSE, echo=F}
# we will save this data to the 'data' folder
download.terror(years.to.survey, "data/") 
dt <- read.terror("data/terror/")
dt <- arrange(dt, Date)
na.index <- which(apply(dt, 1, function(x){ all(is.na(x)) }))
if(length(na.index)!=0) dt <- dt[-na.index, ]
write.csv(dt, file="output/terror raw data.csv", na="", row.names=F)

## Tourism data

dt.tour <- download.tourism("data/")

## Economic data

dt.eco <- download.economy("data/")
dt.eco <- make.economy.great.again(dt.eco)

## OECD data on hours worked

dt.oecd <- get_dataset("ANHRS")
decode  <- get_decoder("data/")  

# Decoding country names

dt.oecd <- ddply(dt.oecd, ~COUNTRY + obsTime + EMPSTAT, function(xframe){
  
  xframe <<- xframe
  real.name <-  decode[decode$CODE==xframe$COUNTRY[1], "Country"]
  if(length(real.name)==0) real.name <- xframe$COUNTRY[1]
  xframe$COUNTRY <- real.name
  return(xframe)
  
})

dt.oecd <- dt.oecd[dt.oecd$EMPSTAT=="TE", ]
dt.oecd[dt.oecd$COUNTRY=="KOR", "COUNTRY"] <- "South Korea"
# dt.oecd[dt.oecd$COUNTRY=="Slovak Republic", "COUNTRY"] <- "Slovakia"
```

```{r data for modelling, include=FALSE, echo=F}

# In this section we will be creating a data frame for a VAR model or a panel model

master.data <- aggregate.terror.by.country(dt)

dt.tour <- rename(dt.tour, c("Year" = "Date", "CountryName" = "Country"))
dt.eco <- rename(dt.eco, c("Year" = "Date", "CountryName" = "Country"))

dt.tour$Country <- gsub("Korea, Rep.", "South Korea", dt.tour$Country)
dt.eco$Country <- gsub("Korea, Rep.", "South Korea", dt.eco$Country)

master.data <- merge(master.data, dt.tour)

for(prod in unique(dt.eco$Indicator)){

  tmp <- dt.eco[dt.eco$Indicator==prod, c("Country", "Date", "value")]
  tmp <- rename(tmp, c("value" = prod))
  master.data <- merge(master.data, tmp)

}

# Merging with OECD data

dt.oecd <- plyr::rename(dt.oecd, c("COUNTRY" = "Country", "obsTime" = "Date", "obsValue" = "Working hours"))
dt.oecd <- dt.oecd[, c("Country", "Date", "Working hours")]

master.data <- merge(master.data, dt.oecd)

# Dropping incomplete rows

master.data <- master.data[complete.cases(master.data), ]

master.data <- arrange(master.data, Country)

# Adding number of neighbours

neighbours <- find.neighbours(unique(master.data$Country), path=path)

master.data <- merge(master.data, neighbours, sort=F)
master.data$Numb.neighbour <- as.numeric(as.character(master.data$Numb.neighbour))


master.data <- plyr::rename(master.data, c('GDP per capita (current US$)' = "GDP.per.capita"))
master.data <- plyr::rename(master.data, c('Working hours' = "Hours.worked"))

```

\section{Countries}

The countries which will be analyzed are all members of the OECD (\textbf{Organisation for Economic Co-operation and Development}). Currently there are 35 member states but in this analysis we will be adding Lithuania and Costa Rica because it is strongly believed that these countries will soon join OECD.

```{r map of OECD countries, echo=F, fig.width=8, fig.height=8, fig.cap= "Map of the OECD countries", echo=F, warning=F, include=T}

OECD.cn <- data.frame(country = unique(master.data$Country), 
                      OECD = rep.int("OECD countries", 
                                     length(unique(master.data$Country))))
malMap <- joinCountryData2Map(OECD.cn, joinCode = "NAME",
  nameJoinColumn = "country", verbose = F, suggestForFailedCodes = F)

mapCountryData(malMap, nameColumnToPlot="OECD", catMethod = "categorical",
  missingCountryCol = gray(.8), oceanCol = 'cyan', mapTitle = "Political map of the world")


```

\newpage

\subsection{Total number of arrivals}

To have a better picture of the tourist flows to the OECD countries we can visualize the data.

```{r arrivals visualization, echo=F, fig.cap="Total number of tourist arrivals", fig.height=10, fig.width=11, fig.pos="H"}

dt.a <- master.data[, c("Country", "Date", "Total.Arrivals"), ]

dt.a$Date <- as.numeric(dt.a$Date)
dt.a$Total.Arrivals <- dt.a$Total.Arrivals/1000

p <- ggplot(dt.a, aes(x=Date, y=Total.Arrivals, group=Country, colour=Country)) + geom_line(size=1.2) + ylab("Total Arrivals, thousands") + theme_bw()  + theme(legend.position="none") + scale_x_continuous(expand = c(0, 3)) + geom_dl(aes(label = Country), method = list(dl.combine("last.bumpup"), cex = 1)) 

plot(p)

```

To see the overal trend from all of these countries is quite difficult so we will plot the average of tourist arrivals in each year.

```{r arrival trend, echo=F, fig.cap="Trend of tourists arrivals", fig.height=6, fig.width=7,  fig.pos="H"}

dt.a <- master.data[, c("Country", "Date", "Total.Arrivals"), ]

dt.a <- ddply(dt.a, ~Date, function(xframe){
  
  xframe <<- xframe
  
  results <- data.frame(Country='OECD', Date=xframe$Date[1], Total.Arrivals=mean(xframe$Total.Arrivals, na.rm=T))
  xframe <- rbind(xframe, results)
  return(xframe[nrow(xframe), ])

})

dt.a$Total.Arrivals <- dt.a$Total.Arrivals/1000
dt.a$Date <- as.numeric(dt.a$Date)

p <- ggplot(dt.a, aes(x=Date, y=Total.Arrivals, group=Country, colour=Country, label=Country)) + geom_line(size=1.2) + ylab("Total Arrivals, thousands") + theme_bw() + scale_x_continuous(expand = c(0, 4)) + geom_dl(aes(label = Country), method = list(dl.combine("first.polygons"), cex = 1)) + geom_dl(aes(label = Country), method = list(dl.combine("last.polygons"), cex = 1)) + stat_smooth(se = FALSE, method = "lm", formula = y ~ ns(x,2),aes( colour="Square trend"), linetype="dashed")  + scale_colour_manual(values = c("coral3", "darkblue"),
                       guide = guide_legend(override.aes = list(
                         linetype = c("solid", "dashed"),
                         shape = c(16, 16)), title="Legend"))

plot(p)

```

From figure 3 we can clearly see that the overal mean through the years is increasing rapidly. In this paper we will try to model these arrival values and identify the biggest factors of this process. 

\newpage
\section{Economic variable selection}

The aim of this paper is to create a model to help explain and forecast the total number of arrivals of tourists to a given OECD country. We want to find the function \textbf{f} in the equation:

\[
log(A)_{it}  = \textbf{f}(log(GDP)_{it}, terror_{it}, log(hours)_{it}, neighbours_{i})
\]

Here 

$A_{it}$ - total number of arrivals to a country i at time period t, thousands

$GDP_{it}$ - GDP per capita at a country i at time period t, thousands 

$terror_{it}$ - total number of terrorist attacks at country i at time period t

$hours_{it}$ - average numbers of hours worked annualy of country's i 5 closest neighbors at time period t

$neighbours_{i}$ - a factor variable indicating whether a country has a "good" geographical location or not. The measure of goodness is whether a country has at least 5 or more surounding countries which the smallest distance to country i is smaller than 100 km.


<!-- \subsection{Motivation behind variable selection} -->

\newpage

\subsection{Gross domestic product per capita}

The variable $GDP$ was chosen to reflect the economic situation and the standart of living in a given country. GDP per capita is calculated by taking the  total output of a country and dividing it by the number of people in the country. The division by the number of people helps us to compare economies between countries. 

```{r OECD GDP per capita, echo=F, fig.cap="GDP per capita of OECD", fig.height=6, fig.width=7,  fig.pos="H"}

dt.a <- master.data[, c("Country", "Date", "GDP.per.capita")]

dt.a <- ddply(dt.a, ~Date, function(xframe){
  
  xframe <<- xframe
  
  results <- data.frame(Country='OECD', Date=xframe$Date[1], GDP.per.capita=mean(xframe$GDP.per.capita, na.rm=T))
  xframe <- rbind(xframe, results)
  return(xframe[nrow(xframe), ])

})

dt.a <- rename(dt.a, c("GDP.per.capita" = "value"))
dt.a$Date <- as.numeric(dt.a$Date)

p <- ggplot(dt.a, aes(x=Date, y=value, group=Country, colour=Country, label=Country)) + geom_line(size=1.2) + ylab("GDP per capita, USD") + theme_bw() + scale_x_continuous(expand = c(0, 4)) + geom_dl(aes(label = Country), method = list(dl.combine("first.polygons"), cex = 1)) + geom_dl(aes(label = Country), method = list(dl.combine("last.polygons"), cex = 1))  + stat_smooth(se = FALSE, method = "lm", formula = y ~ x,aes( colour="Linear trend"), linetype="dashed") +
scale_colour_manual(values = c("darkblue", "coral3"),
                       guide = guide_legend(override.aes = list(
                         linetype = c("dashed", "solid"),
                         shape = c(16, 16)), title="Legend")) 

plot(p)

```

\newpage

\subsection{Number of terrorist attacks}

In recent years there has been an increase in terrorist attacks in the OECD member countries: France, Germany, Turkey and others have suffered attacks, especially from the group called ISIS. It is quite straightforward to think that in light of these attacks tourists will be trying to avoid countries with a higher chance of a terrorist attack. We will want to know whether terrorist attacks is a significant variable to tourist arrivals and how much of an impact it has. 

```{r visualizing terror, echo=F, fig.cap="Terrorism in OECD countries", fig.height=6, fig.width=7,  fig.pos="H"}

dt.a <- master.data[, c("Country", "Date", "Terror.attacks")]

dt.a <- ddply(dt.a, ~Country, function(xframe){
  
  xframe <<- xframe
  
  results <- data.frame(Country=xframe$Country, Date="Total", Terror.attacks=sum(xframe$Terror.attacks, na.rm=T))
  xframe <- rbind(xframe, results)
  return(xframe[nrow(xframe), ])

})

dt.a <- t(dt.a[, c("Country", "Terror.attacks")])
# dt.a[2, ] <- as.numeric(dt.a[2, ])
nn <- dt.a[1, ]
dt.a <- dt.a[-1, ] %>% as.data.frame() %>% t()
dt.a <- apply(dt.a, c(1,2), function(x) {as.numeric(x)})
names(dt.a) <- nn
dt.a <- sort(dt.a)
par(mai=c(1,2,1,1))
barplot(dt.a , col='deepskyblue3',  horiz=T , las=1, cex.names = 0.7, main="Total number of attacks")

```

\newpage

\subsection{Average annaul working hours}

Most countries in the developed world have seen average hours worked decrease significantly. This is due because of the effectiveness of new tools and technologies, overall population growth and other factors.

```{r visualizing working hours, echo=F, fig.cap="Hours worked in OECD countries", fig.height=6, fig.width=7,  fig.pos="H"}

dt.a <- master.data[, c("Country", "Date", "Hours.worked")]

dt.a <- ddply(dt.a, ~Date, function(xframe){
  
  xframe <<- xframe
  
  results <- data.frame(Country='OECD', Date=xframe$Date[1], Hours.worked=mean(xframe$Hours.worked, na.rm=T))
  xframe <- rbind(xframe, results)
  return(xframe[nrow(xframe), ])

})

dt.a <- rename(dt.a, c("Hours.worked" = "value"))
dt.a$Date <- as.numeric(dt.a$Date)

p <- ggplot(dt.a, aes(x=Date, y=value, group=Country, colour=Country, label=Country)) + geom_line(size=1.2) + ylab("Average hours worked annualy") + theme_bw() + scale_x_continuous(expand = c(0, 4)) + geom_dl(aes(label = Country), method = list(dl.combine("first.polygons"), cex = 1)) + geom_dl(aes(label = Country), method = list(dl.combine("last.polygons"), cex = 1)) + stat_smooth(se = FALSE, method = "lm", formula = y ~ x,aes( colour="Linear trend"), linetype="dashed") +
scale_colour_manual(values = c("darkblue", "coral3"),
                       guide = guide_legend(override.aes = list(
                         linetype = c("dashed", "solid"),
                         shape = c(16, 16)), title="Legend")) 

plot(p)

```

As we can see from figure 6 the overal downwards trend of the working annaul working hours' average is true to the OECD countries.

```{r creating a new hours worked variable, include=FALSE, echo=F}

## For this analysis we will be creating a new variable regarding the average of hours worked in the neighboring countries of a given country
## This variable helps to explain the impact of the decline in the average working hours for the total number of arrivals to a given country

dmat <- read.csv(paste0(path, "GeoCountries/distances.csv"), header = F)
colnames(dmat) <- dmat[1, ]
rownames(dmat) <- dmat[1, ]
dmat <- dmat[-1, -1] 

dmat <- as.data.frame(dmat)
  
names.on.file <- colnames(dmat)
decode <- merge(data.frame(cown=as.numeric(names.on.file)), countrycode_data[, c("country.name.en", "cown")], sort=F)
  
decode$country.name <- gsub("Republic of Korea", "South Korea", decode$country.name)
decode$country.name <- gsub("Slovakia", "Slovak Republic", decode$country.name)
decode$country.name <- gsub("United Kingdom of Great Britain and Northern Ireland", "United Kingdom", decode$country.name)  
decode$country.name <- gsub("United States of America", "United States", decode$country.name)  

colnames(dmat) <- decode$country.name
rownames(dmat) <- decode$country.name


# leaving only OECD countries

dmat <- dmat[, which(colnames(dmat) %in% unique(master.data$Country))]
dmat <- dmat[which(rownames(dmat) %in% unique(master.data$Country)), ]

master.data <- ddply(master.data, ~Country, function(xframe){
  
  xframe <<- xframe
  
  # friend <- rownames(dmat)[which(dmat[, xframe$Country[1]]<500)] 
  # 
  # friend <- friend[-which(friend==xframe$Country[1])]
  #  
  # if(length(friend)==0){
    
    friend <- rownames(dmat)[which(dmat[, xframe$Country[1]] %in% sort(dmat[, xframe$Country[1]])[1:5])] 

    friend <- friend[-which(friend==xframe$Country[1])]
     
  # }

  new.ave <- master.data[master.data$Country %in% friend, ]
  
  new.ave <- ddply(new.ave, ~Date, function(yframe){
    
    yframe <<- yframe
    
    yframe[nrow(yframe), "Hours.worked"] <- mean(yframe$Hours.worked)
    
    yframe <- yframe[nrow(yframe), ]
    yframe$Country <- "OECD"
  
    return(yframe)  
  })
  
  dates <- intersect(new.ave$Date, xframe$Date)
  xframe[xframe$Date %in% dates, "Hours.worked"] <- new.ave[new.ave$Date %in% dates, "Hours.worked"]
  return(xframe)
  
})


```

\newpage

\section{Modelling}

\subsection{Panel data model}

Panel data provides information on individual behaviour, both across individuals and over time - the data has cross-sectional and time-series dimensions. Panel data includes N individuals over T regular time periods. We can denote the model simmilary like a typical bivariate reggression model:

\[
\tag{1}
  y_{it} = \alpha_{i} + \beta X_{it} + u_{it}
\]

The difference from a typical reggression model is that there are the $i$ and the $t$ terms, where $i$ denotes the individual and $t$ denotes the time period. The construction of a panel model starts with appropriately 'stacking' the data set. Assuming that y is the dependant variable and X is the matrix of the explanatory variables we need to have the following structure: 

\[
  Y = y_{i} = \begin{bmatrix}
         y_{1}    \\
         y_{2}    \\[0.3em]
         \vdots   \\[0.3em]
         y_{N}     
  \end{bmatrix} = \begin{bmatrix}
         e    \\
         0    \\[0.3em]
         \vdots   \\[0.3em]
         0     
  \end{bmatrix}  \alpha_{1} + \begin{bmatrix}
       0    \\
       e    \\[0.3em]
       \vdots   \\[0.3em]
       0    
\end{bmatrix}  \alpha_{2} + \cdots + \begin{bmatrix}
       0    \\
       0    \\[0.3em]
       \vdots   \\[0.3em]
       e     \\[0.3em]
\end{bmatrix}  \alpha_{N} + \begin{bmatrix}
       x_{1}    \\[0.3em]
       x_{2}    \\[0.3em]
       \vdots   \\[0.3em]
       x_{n}     \\[0.3em]
\end{bmatrix} \beta +   \begin{bmatrix}
       u_{1}    \\[0.3em]
       u_{2}    \\[0.3em]
       \vdots   \\[0.3em]
       u_{n}     \\[0.3em]
\end{bmatrix}
\]

Here

\[
y_{i} = \begin{bmatrix}
       y_{i1}    \\
       y_{i2}    \\[0.3em]
       \vdots   \\[0.3em]
       y_{iT}     \\[0.3em]
\end{bmatrix} x_{i} = \begin{bmatrix}
       x_{1i1} & x_{2i1} & \cdots & x_{Ki1}    \\
       x_{1i2} & x_{2i2} & \cdots & x_{Ki2}   \\[0.3em]
       \vdots & \vdots & \cdots & \vdots   \\[0.3em]
      x_{1iT} & x_{2iT} & \cdots & x_{KiT}      \\[0.3em]
\end{bmatrix}
\]

For example, the 'head' and the 'tail' of the raw data which will me analysed looks like this:

```{r visualising data table, echo=F}

pdata <- plm.data(master.data, index = c("Country", "Date")) 

pdata$Total.Arrivals <- log(pdata$Total.Arrivals)
pdata$GDP.per.capita <- log(pdata$GDP.per.capita)
pdata$Hours.worked   <- log(pdata$Hours.worked)

## Creating a factor indicating a "good" geographical location

pdata$Numb.neighbour <- sapply(pdata$Numb.neighbour, function(x){
  
  if(x < 5) x <- 0
  if(x >=5) x <- 1
  return(x)
  
})

pdata$Numb.neighbour <- as.factor(pdata$Numb.neighbour)
kable(head(pdata), format = "latex", row.names = F)
kable(tail(pdata), format = "latex", row.names = F)

```


```{r pooling panel model, include=FALSE, echo=F}

model.ols <- lm(formula = Total.Arrivals ~ Terror.attacks + GDP.per.capita + Hours.worked + Numb.neighbour , data=pdata)
# summary(model.ols)
```

\subsection{Pooling estimator}

The first model which we will be examining will be the simple OLS model which ignores the panel data structure. The equation is then:

\[
y = \alpha + X \beta + \epsilon
\]

And the estimator is derived solving the equation:

\[
\widehat{\beta} = \left( X X^{T} \right)^{-1}X^{T}y
\]


```{r, results='asis', echo=F, warning=F, fig.pos="h"}
  texreg(model.ols, caption = "OLS model", custom.coef.names = c("Intercept", "log attacks", "log GDP", "log Hours", "neighbours"))
```

According to this model, there is a positive influence to tourism caused by terrorist attacks, which is not logical. Given that this model does not take into consideration the panel structure of the data, we will not be using straighforward OLS in our analysis. 

\newpage

\subsection{Fixed effects model}

In this section we will be creating a fixed effects panel data model. Recall equation (1). The $\alpha_{i}$ is called the \textit{individual effect}.
If $\alpha_{i}$ does correlate with $X_{it}$ then the panel model can be reffered to as a fixed effects model. 

```{r fixed effects model, include=FALSE, echo=F}

model.fixed <- plm(formula = Total.Arrivals ~ Terror.attacks + GDP.per.capita + Hours.worked + Numb.neighbour , data=pdata, model="within")
# summary(model.fixed)
```

```{r, results='asis', echo=F, warning=F, fig.pos="h"}
  texreg(model.fixed, caption = "Fixed effects model", custom.coef.names = c("log attacks", "log GDP", "log Hours"), digits = 3)
```

As we can see from table 2 now the coefficients are logical - one terrorist attack can lead to about 0.5 percent decline in tourist arrivals. A one percent increase in GDP per capita can lead up to 0.5 increase in tourist arrivals and decreasing 1 percent of hours worked within neighboring countries leads to 0.43 percent more tourist arrivals. Table 2 does not include the intercept term and the the factor variable regarding its "goodness" of geographical place. 

The intercept is missing because every country &\textbf{i}& has it's own intercept value and those values are not displayed in model output. This is the main goal of the fixed effects model because we get a different level for every country thus we take into account the heterogeneity across countries and time.

The factor variable is missing because it does not change over time and the overall effect is 'transfered' to the intercept. 

Applying the F test for individual effects gives the result:

```{r pFtest, echo=T, comment=NA}
  pFtest(model.fixed, model.ols)
```

According to these results the fixed effects model is better in explaining the number of arrivals compared to the OLS model.

```{r estimating ols and fixed model fits, echo=F, comment=NA}
  
  pdata <- ddply(pdata, ~Country + Date, function(xframe){
    
    xframe <<- xframe
    
    
    ## fixed effects
    
    intercept <- fixef(model.fixed)[xframe$Country[1]] %>% as.numeric()
    X <- xframe[, c('Terror.attacks', "GDP.per.capita", "Hours.worked")]
    fit <- model.fixed$coefficients[1] * X[1] + model.fixed$coefficients[2] * X[2] + model.fixed$coefficients[3] * X[3] + intercept
    fit <- as.numeric(fit) # this is the fit of the logged number of arrivals
    xframe$fc.fixed.effect <- fit
    
    ## OLS
    
    X <- xframe[, c('Terror.attacks', "GDP.per.capita", "Hours.worked", "Numb.neighbour")] %>% as.numeric()
    fit <- model.ols$coefficients[2] * X[1] + model.ols$coefficients[3] * X[2] + model.ols$coefficients[4] * X[3] + model.ols$coefficients[5] * X[4] 
    fit <- fit + model.ols$coefficients[1] 
    xframe$fc.OLS <- fit
    
    return(xframe)
  })

```


```{r visualizing the differences between fits, echo=F, fig.cap="OLS and fixed effects model comparison", fig.height=7, fig.width=8,  fig.pos="H"}
  
fdt <- pdata[, c("Country", "Date", "Total.Arrivals", "fc.fixed.effect", 'fc.OLS')]
fdt$Total.Arrivals  <- exp(fdt$Total.Arrivals)/1000
fdt$fc.fixed.effect <- exp(fdt$fc.fixed.effect)/1000
fdt$fc.OLS          <- exp(fdt$fc.OLS)/1000
fdt$Date <- fdt$Date %>% as.character() %>% as.numeric()

## we will plot 4 countries in order to have a clear picture
cn <- c( "Australia",  "Austria", "Chile", "Lithuania")
points <- data.frame()

par(mfrow=c(2,2))
for(cc in cn){
  
  tmp <- fdt[fdt$Country==cc, ]
  points <- tmp[, c("Total.Arrivals", "fc.fixed.effect", 'fc.OLS')]
  
grid.frame(x=as.numeric(tmp$Date), y=points)
  matplot(x=as.numeric(tmp$Date), y=points, lwd=1, lty=1, cex=1.25, pch=20, xlab="Time", ylab="Arrivals, thousands", add = T, type="o",
        col=c('salmon1', "firebrick1", 'cornflowerblue'))
  legendary2(c("Original", "Fixed effect fit", "OLS fit"), col=c('salmon1', "firebrick1", 'cornflowerblue'))
  mtext(cc, col="blueviolet", line=2, cex=1.25, adj = 0)
}

```

\newpage

\subsection{Statistics for accuracy}

We can measure the goodness of fit using several statistics. We will use the the \textit{mean absolute error} (MAE), \textit{mean squared error} (MSE)
and the \textit{mean absolute percentage error} (MAPE). These statistics are defined by:

\[
MSE = \dfrac{1}{n}  \sum_{i=1}^{n}\left( y - \widehat{y} \right)^{2}
\]

\[
MAE = \dfrac{1}{n}  \sum_{i=1}^{n}\left| y - \widehat{y} \right|
\]

\[
MAPE = \dfrac{100}{n}  \sum_{i=1}^{n}\left| \dfrac{y - \widehat{y}}{y} \right| 
\]

Here

$\widehat{y}$ - fitted value

$y$ - original value


$n$ - number of observations


\begin{center}

```{r forecast accuracy, echo=F, results= "asis", fig.pos="H"}
  
fdt <- pdata[, c("Country", "Date", "Total.Arrivals", "fc.fixed.effect", 'fc.OLS')]
fdt$Total.Arrivals  <- exp(fdt$Total.Arrivals)
fdt$fc.fixed.effect <- exp(fdt$fc.fixed.effect)
fdt$fc.OLS          <- exp(fdt$fc.OLS)

accuracy.table <- matrix(ncol=2, nrow=3) %>% as.data.frame()
colnames(accuracy.table) <- c("OLS", "Fixed effects")
rownames(accuracy.table) <- c("MSE", "MAPE", "MAE")

## MSE:

accuracy.table[1, 1] <- MSE(fdt$Total.Arrivals, fdt$fc.OLS)
accuracy.table[1, 2] <- MSE(fdt$Total.Arrivals, fdt$fc.fixed.effect)

## MAPE

accuracy.table[2, 1] <- MAPE(fdt$Total.Arrivals, fdt$fc.OLS)
accuracy.table[2, 2] <- MAPE(fdt$Total.Arrivals, fdt$fc.fixed.effect)

## MAE

accuracy.table[3, 1] <- MAE(fdt$Total.Arrivals, fdt$fc.OLS)
accuracy.table[3, 2] <- MAE(fdt$Total.Arrivals, fdt$fc.fixed.effect)

# options("scipen" = 5)

accuracy.table <- apply(accuracy.table, c(1, 2), function(x) {
  # x <- format(x,scientific=FALSE)
  x <- as.numeric(x)
  x <- round(x, 3)
  return(x)
  })


kable(accuracy.table, format = "latex", row.names = T, caption = "Table for accuracy")
```

\end{center}

As we can see from figure 7 and the statistics for accuracy table the fixed effects models' predictions are much closer to the original values. 

\newpage

\subsection{Random effects model}

There are two main types of panel models: the fixed effects model and the random effects models. The main difference between them is the assumption of $\alpha_{i}$ regarding the regressor matrix $X_{it}$. In the random effects model case it is assumed that $\alpha_{i}$ and $X_{it}$ do not correlate.

```{r random effects model, include=FALSE, echo=F}

model.random <- plm(formula = Total.Arrivals ~ Terror.attacks + GDP.per.capita + Hours.worked, data=pdata, model="random")
# summary(model.random)
```
```{r, results='asis', echo=F, warning=F, fig.pos="h"}
  texreg(model.random, caption = "Random effects model", custom.coef.names = c("intercept", "log attacks", "log GDP", "log Hours"), digits = 3)
```

The categorical variable \textit{numb.neighbour} has been dropped due to insignificance. Comparing tables 2 and 4 we can see that the signs near the coefficients are the same and the actual values of the estimators are almost even. The random effects model has a little higher R statistic. The main difference here is that there is the intercept term and we lose some "uniqueness" to the countries which was present with the fixed effects model. One measure to determine which model is better for the given data is the Hausman test. 

```{r pHtest, echo=T, comment=NA}
  phtest(model.random, model.fixed)
```

As we can see from the test result we cannot reject the null hypothesis therefore we conclude that the random effects coefficients are more efficient and are consistent. Therefore the test implies that we sould use the random effects model.

\subsection{Fixed effects versus random effects}

As we saw in the previous section the Hausman test for panel data indicated that we should use the random effects model. The test is only one of the criteria for deciding which model to use. 

```{r residuals of models, echo=F, fig.cap="Residuals of models", fig.height=7, fig.width=8,  fig.pos="H"}
  
par(mfrow=c(1,2))

res.random <- residuals(model.random)
res.fixed  <- residuals(model.fixed)
  
grid.frame(x=seq(from=1, length.out = length(res.random)), y=as.numeric(res.random))
matplot(x=seq(from=1, length.out = length(res.random)), y=as.numeric(res.random), lwd=1, lty=1, cex=1.25, pch=20, ylab="Residual value", xlab="Index", type="o", add=T, col=c('salmon1'))
mtext("Random effects model residuals", col="salmon1", line=1, cex=1.25, adj = 0)

grid.frame(x=seq(from=1, length.out = length(res.random)), y=as.numeric(res.random))
matplot(x=seq(from=1, length.out = length(res.fixed)), y=as.numeric(res.fixed), lwd=1, lty=1, cex=1.25, pch=20, ylab="Residual value", xlab="Index", type="o", col=c('cornflowerblue'), add=T)
mtext("Fixed effects model residuals", col="cornflowerblue", line=1, cex=1.25, adj = 0)

```


As we can see in figure 8 the residuals visually look very similar. 


```{r estimating random model fits, echo=F, comment=NA}
  
  pdata <- ddply(pdata, ~Country + Date, function(xframe){
    
    xframe <<- xframe
    
    ## random effects
  
    X <- xframe[, c('Terror.attacks', "GDP.per.capita", "Hours.worked")]
    coef <- model.random$coefficients
    fit <- coef[1] + coef[2] * X[1] + coef[3]* X[2] + coef[4] * X[3] 
    fit <- as.numeric(fit) # this is the fit of the logged number of arrivals
    xframe$fc.random.effect <- fit
    
    return(xframe)
  })

```

```{r accuracy statistics of the models,  echo=F, comment=NA}
  
fdt <- pdata[, c("Country", "Date", "Total.Arrivals", "fc.fixed.effect", 'fc.random.effect')]
fdt$Total.Arrivals    <- exp(fdt$Total.Arrivals)
fdt$fc.fixed.effect   <- exp(fdt$fc.fixed.effect)
fdt$fc.random.effect  <- exp(fdt$fc.random.effect)

accuracy.table <- matrix(ncol=2, nrow=3) %>% as.data.frame()
colnames(accuracy.table) <- c("Random effects", "Fixed effects")
rownames(accuracy.table) <- c("MSE", "MAPE", "MAE")

## MSE:

accuracy.table[1, 1] <- MSE(fdt$Total.Arrivals, fdt$fc.random.effect)
accuracy.table[1, 2] <- MSE(fdt$Total.Arrivals, fdt$fc.fixed.effect)

## MAPE

accuracy.table[2, 1] <- MAPE(fdt$Total.Arrivals, fdt$fc.random.effect)
accuracy.table[2, 2] <- MAPE(fdt$Total.Arrivals, fdt$fc.fixed.effect)

## MAE

accuracy.table[3, 1] <- MAE(fdt$Total.Arrivals, fdt$fc.random.effect)
accuracy.table[3, 2] <- MAE(fdt$Total.Arrivals, fdt$fc.fixed.effect)

# options("scipen" = 5)

accuracy.table <- apply(accuracy.table, c(1, 2), function(x) {
  # x <- format(x,scientific=FALSE)
  x <- as.numeric(x)
  x <- round(x, 3)
  return(x)
  })


kable(accuracy.table, format = "latex", row.names = T, caption = "Random vs fixed model accuracy")


```

```{r visualizing the differences between random and fixed fits, echo=F, fig.cap="OLS and fixed effects model comparison", fig.height=7, fig.width=8,  fig.pos="H"}
  
fdt <- pdata[, c("Country", "Date", "Total.Arrivals", "fc.fixed.effect", 'fc.random.effect')]
fdt$Total.Arrivals  <- exp(fdt$Total.Arrivals)/1000
fdt$fc.fixed.effect <- exp(fdt$fc.fixed.effect)/1000
fdt$fc.random.effect         <- exp(fdt$fc.random.effect)/1000
fdt$Date <- fdt$Date %>% as.character() %>% as.numeric()

## we will plot 4 countries in order to have a clear picture
cn <- c( "Australia",  "Austria", "Chile", "Lithuania")
points <- data.frame()

par(mfrow=c(2,2))
for(cc in cn){
  
  tmp <- fdt[fdt$Country==cc, ]
  points <- tmp[, c("Total.Arrivals", "fc.fixed.effect", 'fc.random.effect')]
  
grid.frame(x=as.numeric(tmp$Date), y=points)
  matplot(x=as.numeric(tmp$Date), y=points, lwd=1, lty=1, cex=1.25, pch=20, xlab="Time", ylab="Arrivals, thousands", add = T, type="o",
        col=c('salmon1', "firebrick1", 'cornflowerblue'))
  legendary2(c("Original", "Fixed effect fit", "random effect fit"), col=c('salmon1', "firebrick1", 'cornflowerblue'))
  mtext(cc, col="blueviolet", line=2, cex=1.25, adj = 0)
}

```

Inspecting the accuracy of the forecasts (figure 9 and table 4) we can clearly see that the fixed effects model has an advantage. This is because the intercept for each of the countries is different in the fixed effects model thus giving us a more precise model for each individual country. Having this in mind we may conclude that the fixed effect model is $\textit{better}$ in describing the total number of arrivals for OECD countries.

\newpage

\section{Conclusion}




