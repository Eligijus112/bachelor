---
header-includes:
  - \usepackage[L7x]{fontenc}
  - \bibliographystyle{plainnat}
output:
 pdf_document: 
    fig_caption: yes
    keep_tex: yes
    number_sections: no
    latex_engine: pdflatex
margin-left: 6.5in
margin-top: 9in
fontsize: 12pt    
papersize: A4
fontfamily: palatino
---

```{r setup, include=FALSE}
library(plm)
library(RCurl)
library(dplyr)
library(plyr)
# library(emibase)
library(plotrix)
# library(mgcv)
library(reshape2)
library(readxl)
library(knitr)
library(rworldmap)
library(cshapes)
library(countrycode)
library(texreg)
library(ggplot2)
library(directlabels)
# library(splines)
library(MASS)
library(TTR)
library(rsdmx)
library(httr)
library(splines)
library(xtable)
# library(dynpanel)
library(devtools)
library(corrplot)
source('functions.R')
library(car)
library(tseries)
library(RefManageR)
```

\vskip 20pt
\centerline{\bf \large VILNIUS UNIVERSITY}
\bigskip
\centerline{\large \textbf{FACULTY OF MATHEMATICS AND INFORMATICS}}
\vskip 120pt
\centerline{\bf \Large \textbf{BACHELOR THESIS}}
\vskip 50pt
\begin{center}
{\bf \LARGE Tourism model in the Western world}

\vspace{4mm}

\end{center}
\vskip 120pt
\centerline{\Large Eligijus Bujokas}
\vskip 120pt
\centerline{\large \textbf{VILNIUS (2016.12.07)}}

\newpage

\begin{titlepage}
\centerline {\bf \large FACULTY OF MATHEMATICS AND INFORMATICS}
\centerline {\bf DEPARTMENT OF MATHEMATICAL ANALYSIS}
\vskip 120pt
\large Scientific supervisor Vaidotas Zemlys - Balevičius, PhD \underline{\hskip 95pt}
\vskip 150pt
\end{titlepage}

\newpage

\begin{center}{\large\textbf{Turizmo modelis vakarų šalyse}}\end{center}
\vspace{2\baselineskip}
\begin{center}\textbf{Santrauka}\end{center}

Šio darbo tikslas yra sudaryti turizmo modelį Vakarų Europos šalims bei Amerikai ir Kanadai. Naudojantis viešais pasaulinio banko (World Bank), OECD ir Merilando Universiteto duomenų bazės (the Department of Homeland Security Center of Excellence led by the Maryland University) duomenimis bandėme paaiškinti kasmetinius atvykusių turistų srautų procentinius pokyčius naudodamiesi procentiniais BVP vienam asmeniniui pokyčiais, teroro aktų skaičiaus pokyčiais, mokesčių paslaugoms pokyčiais, kasmetinių dirbamų valandų vidurkių pokyčiais, investicijų į infrastruktūrą pokyčiais bei pačių turistų srautų ankstinių pokyčiais. Ryšio nustatymui naudojome paprastą tiesinį, fiksuotų efektų, atsitiktinių efektų bei dinaminius panelinius modelius. Tokio tipo modelių pasirinkimą lėmė erdvinė blokuotų originalių duomenų struktūra. Išbandę įvairius kintamųjų ankstinius, modelių tipus, pritaikę pažingsninį kintamųjų išmetimo (išmetant mažiausiai reikšmingą) metodą gavome, jog vienintelis kintamasis, darantis reikšmingą įtaką turistų srautų pokyčiams buvo metinis vidutinis pradirbtų valandų skaičiaus pokytis. Pašalinę išskirtis gavome tikslesnius rezultatus: reikšmingą įtaką turi metinis vidutinis pradirbtų valandų skaičiaus pokytis, teroristinių įvykių skaičiaus pokytis, mokesčių paslaugų sektoriui pokytis (skaičiuojamas procentu nuo BVP) bei atvykusių turistų skaičiaus procentinio pokyčio ankstinys. Pasinaudoję visa išskaidymo informacija, kurią suteikia OECD duomenų bazė ir pasinaudoję tiesiniais modeliais gavome, jog lyginant užsienio ir vietinę ekonomikas, reikšmingą įtaką turistų skaičiaus pokyčiams turi šalių, iš kurių atvyksta turistai, BVP vienam asmeniui pokyčiai. BVP vienam asmeniui pokyčiai šalyje, į kurią atvyksta turistai, nėra reikšmingi norint paaiškinti turizmo srautus. 

\vspace{\baselineskip}
\noindent\textbf{Raktiniai žodžiai :}
 BVP, OECD, pasaulinis bankas, panelinis modelis, darbo valandos, terorizmas, turizmas, mokesčiai paslaugoms, investicijos į infrastruktūrą, tiesiniai modeliai.

\vspace{\baselineskip}
\thispagestyle{empty}

\newpage

\begin{center}{\large\textbf{Tourism model in western countries}}\end{center}

\vspace{2\baselineskip}
\begin{center}\textbf{Abstract}\end{center}
The aim of this paper is to construct a model to help explain the number of tourist arrivals in Western European countries and USA and Canada. By using public data from the World Bank, OECD and the Department of Homeland Security Center of Excellence led by the Maryland University's database we tried to explain the differences of tourist arrivals to a given country by using changes in GDP per capita, differences in terror attacks, differences in taxes on good and services, changes in investments to infrastructure, changes in average annual hours worked and the lags of changes of tourist arrivals. To identify the relationship we used linear, fixed-effects, random effects and dynamic panel models. We selected these types of models because of the cross-sectional structure of the data. After trying various lags of variables, different types of models and the step-wise elimination of variables algorithm we have concluded that the only significant variable to the changes in tourist flows is the changes in annual hours worked. After removing the outliers more precise results were gotten: changes in annual hours worked, changes in terror attacks, changes in taxes on goods and services and the lag of percentage change in tourist flows all got a significant influence to the changes in tourism flows. After using the information provided by the OECD database regarding the structure (limited) of tourism flows we have found that the only significant variable to the changes of tourism flows is the changes in GDP per capita of the countries from which the tourist arrive. GDP per capita in the country to which the tourist arrive and all of the other egzogenous variables mentioned above do not seem to have a significant impact to the changes to tourist arrivals.

\vspace{\baselineskip}
\noindent\textbf{Key words :}
GDP per capita, OECD, World Bank, panel model, hours worked, terrorism, tourism, taxes on goods and services, investments to infrastructure, linear models 

\vspace{\baselineskip}

\newpage

\tableofcontents


```{r constants, include=FALSE}
years.to.survey <- 1995:2015
path <- "data/"
# regions <- read.csv(file="input/regions.csv", stringsAsFactors = F)
createdir("plots")
# createdir("input")
createdir("data")
createdir("output")
# devtools::install_url("http://cran.r-project.org/src/contrib/rprojroot_1.2.tar.gz") ## MAY RESOLVE TROUBLESHOOTS
knitr::opts_chunk$set(fig.pos = 'h')

bib <- ReadBib("bibliography.bib", check = FALSE)
BibOptions(check.entries = FALSE, style = "markdown", bib.style = "alphabetic", cite.style = 'alphabetic')
```

```{r data download and reading it, include=FALSE, echo=F}
# we will save this data to the 'data' folder
# download.terror(years.to.survey, "data/") 
# dt <- read.terror("data/terror/")
# dt <- arrange(dt, Date)
# na.index <- which(apply(dt, 1, function(x){ all(is.na(x)) }))
# if(length(na.index)!=0) dt <- dt[-na.index, ]
# write.csv(aggregate.terror.by.country(dt), file="input/terror raw data.csv", na="", row.names=F)

countries <- c("France", "United Kingdom", "United States", "Canada", "Germany", "Spain", "Portugal", "Netherlands", "Austria", "Italy", "Belgium")
all.terror <- read.csv("data/terror2/all terror.csv", stringsAsFactors = F)
all.terror <- all.terror[all.terror$iyear > 1995 & all.terror$country_txt %in% countries, c("iyear", "country_txt")]

dt.terror <- ddply(all.terror, ~country_txt + iyear, function(xframe){
  
  xframe <<- xframe
  
  xframe$total <- nrow(xframe)
  
  return(xframe[nrow(xframe), ])
})
 
dt.terror <- ddply(dt.terror, ~country_txt, function(xframe){
  
  xframe <<- xframe
  
  miss <- setdiff(paste(1995:2015), xframe$iyear)
  
  if(length(miss)!=0){
    
    
    add <- matrix(ncol=3, nrow=length(miss)) %>% as.data.frame
    names(add) <- names(xframe)
    add$iyear <- miss
    add$country_txt <- xframe$country_txt[1]
    add$total <- 0
    
    xframe <- rbind(xframe, add) %>% arrange(iyear)
  }
  
  return(xframe)
})
  
dt.terror <- rename(dt.terror, c("iyear" = "Date", "country_txt" = "Country", "total" = "Terror.attacks"))
write.csv(dt.terror, "input/terror raw data.csv", row.names=F, na="")
  
## Tourism data
# 
# dt.tour <- download.tourism("data/")
# write.csv(dt.tour, file="input/total arrivals.csv",  na="", row.names=F)

## Economic data

dt.eco <- download.economy("data/")
dt.eco <- make.economy.great.again(dt.eco)
write.csv(dt.eco, file="input/gdp.csv", na="", row.names=F)

## OECD data on hours worked

dt.oecd <- get_dataset("ANHRS", path="data/")
decode  <- get_decoder("data/")  
write.csv(dt.oecd, file="input/hours.csv", na="", row.names=F)

```

```{r data input, include=FALSE, echo=F}

decoder  <- read.csv("input/decoder.csv", stringsAsFactors = F)
struct <- read.csv("input/tourism structure.csv", stringsAsFactors = F) %>% get.cols
flows  <- read.csv("input/receipts.csv", stringsAsFactors = F) %>% get.cols
terror <- read.csv("input/terror raw data.csv", stringsAsFactors = F)
tax    <- read.csv("input/tax.csv", stringsAsFactors = F) %>% filter(MEASURE=="PC_TOT_TAX") 
gdp    <- read.csv("input/gdp.csv", stringsAsFactors = F) %>% rename(c("CountryName" = "Country"))
hours  <- read.csv("input/hours.csv", stringsAsFactors = F) %>% format.hours(decoder)
Inv    <- read.csv("input/infrastructure.csv", stringsAsFactors = F) %>% filter(SUBJECT=="INLAND")
total.arrivals <- read.csv("input/total arrivals.csv", stringsAsFactors = F)
receipts <- read.csv("input/receiptsUS.csv", stringsAsFactors = F)

```

```{r formating input for the big model, include=FALSE, echo=F}

## Merging everything to one data frame

master.data <- merge(total.arrivals, terror)
gdp.m <- gdp[, c("Country", "Date", "value")] %>% rename(c("value"="gdp"))
master.data <- merge(master.data, gdp.m)

tax <- rename(tax, c("LOCATION" = "ï..LOCATION"))
tax <- tax[, c("ï..LOCATION", "TIME", "INDICATOR",  "Value")]
tax <- rename(tax, c("ï..LOCATION" = "Country", "INDICATOR" = "Indicator", "TIME" = "Date"))
  
tax <- ddply(tax, ~Country, function(xframe){
    
  xframe <<- xframe
  real.name <-  decoder[decoder$CountryCode==xframe$Country[1], "CountryName"]
  if(length(real.name)==0) real.name <- xframe$Country[1]
  xframe$Country <- real.name
  return(xframe)
    
})
  
tax[tax$Country=="Korea, Rep.", "Country"] <- "South Korea"  
tax[tax$Country=="Korea", "Country"] <- "South Korea" 
  
Inv <- rename(Inv, c("LOCATION" = "ï..LOCATION"))
Inv <- Inv[, c("ï..LOCATION", "TIME", "INDICATOR",  "Value")]
Inv <- rename(Inv, c("ï..LOCATION" = "Country", "INDICATOR" = "Indicator", "TIME" = "Date"))
  
Inv <- ddply(Inv, ~Country, function(xframe){
    
  xframe <<- xframe
  real.name <-  decoder[decoder$CountryCode==xframe$Country[1], "CountryName"]
  if(length(real.name)==0) real.name <- xframe$Country[1]
  xframe$Country <- real.name
  return(xframe)
    
})
  
Inv[Inv$Country=="Korea, Rep.", "Country"] <- "South Korea"  
Inv[Inv$Country=="Korea", "Country"] <- "South Korea" 

tax.m <- tax[, c("Country", "Date", "Value")] %>% rename(c("Value"="tax"))
master.data <- merge(master.data, tax.m)
Inv.m <- Inv[, c("Country", "Date", "Value")] %>% rename(c("Value"="Investment"))
master.data <- merge(master.data, Inv.m)
hours.m <- hours[, c("Country", "Date", "Value")] %>% rename(c("Value"="hours.worked"))
master.data <- merge(master.data, hours.m)
master.data <- master.data[complete.cases(master.data), ]

master.data <- ddply(master.data, ~Country, function(xframe){
  
  xframe <<- xframe
  if(nrow(xframe)!=length(master.data$Date %>% unique())){
    
    xframe <- NULL
    
  }
  
  return(xframe)
  
})

master.data <- master.data[master.data$Country %in% countries, ]

pdata <-  plm.data(master.data, index = c("Country", "Date")) 
log.vars <- c("Total.Arrivals", "gdp", "hours.worked")
pdata[, log.vars] <- apply(pdata[, log.vars], c(1, 2), log)

pdata <- ddply(pdata, ~Country, function(xframe){
  
  xframe <<- xframe
  xframe[, 3:dim(xframe)[2]] <- apply(xframe[, 3:dim(xframe)[2]], 2, function(x){
  
  return(c(NA, diff(x)))
  
  })
  
  return(xframe)
  
})

pdata <- pdata[complete.cases(pdata), ]
```
\newpage

\section{Introduction}

Tourism is travel for pleasure; also the theory and practice of touring, the business of attracting, accommodating, and entertaining tourists, and the business of operating tours. According to UNWTO (United Nations World Tourism Organization)`r Citet(bib, 'UNWTO', .opts = list(cite.style = "numeric"))`:

$\bullet$ 1 in 11 jobs worldwide are related to the tourism industry.

$\bullet$ 10 percent of total world GDP is produced by tourism.

$\bullet$ Tourism made up for about 7 percent of world's exports: 1.5 trillion US dollars.

$\bullet$ In 2015 there were 1186 million tourists worldwide and this number is expected to grow up to 1.8 billion by 2030.

$\bullet$ In 2015 the total receipts by tourists was about 1.26 billion US dollars.

From these figures it is clear to see that every country can benefit from an increase in tourism flows. Tourism continues to demonstrate its key role in generating economic activity, employment and export revenues in the OECD area, where it directly contributes, on average, 4.1 \% of GDP, 5.9 \% of employment and 21.3 \% of service exports. Tourism offers strong potential to support job rich growth and at around 80 percent, tourism exports also generate higher than average domestic value added. International tourist arrivals surpassed 1.1 billion in 2014 (World Tourism Organization), following a resurgence in arrivals to OECD countries (6.4 \%), which increased at a faster rate than the global average (4.2 \%) (as of 2016). A large part of countries that are regarded as the "Western World" are members of the OECD organization. In this paper we will hold that the western countries are France, United Kingdom, USA, Canada, Germany, Spain, Portugal, Netherlands, Austria, Italy and Belgium. These countries are chosen because of the volume of free data available for them. 

According to WTTC (World Travel and Tourism Council), the outlook for the Travel & Tourism sector in 2017 remains robust and will continue to be at the forefront of wealth and employment creation in the global economy, despite the emergence of a number of challenging headwinds. Travel & Tourism forecasts over the next ten years also look extremely favourable with predicted growth rates of 3.9% annually `r Cite(bib, 'WTTC', .opts = list(cite.style = "numeric"))`. 

```{r receipts, echo=F, fig.cap="Tourist receipts in the Western countries (World Bank data)", fig.height=5, fig.width=6,  fig.pos="!ht", fig.align="center"}

receipts <- receipts %>% rename(c("ï..Country.Name" = "CountryName", "Indicator.Name" = "Product"))
receipts <- receipts[receipts$CountryName %in% countries, ]
colnames(receipts) <- gsub("X", "", colnames(receipts))

sums <- apply(receipts[, paste(1960:2016)], 2, sum)

sums <- sums[!is.na(sums)]/1000000000

grid.frame(x=as.numeric(names(sums)), y=sums, xlab="Time", ylab="Total receipts, trillion US")

matplot(x=as.numeric(names(sums)), y=sums, pch=19, type="o", col="dodgerblue1", add=T)

mtext("Total receipts from tourists in the selected countries", col="salmon1", line=1, cex = 1.2)
```

From figure 1 it is clear to see that the trend of the total receipts from tourists in the countries which were mentioned is increasing. It is likelly therefore that in the future receipts will only grow in absolute sense so goverments should try and find ways to lure tourists to their countries.

Additionally, acording to the World Economic Forum's report on tourism `r Cite(bib, 'WEC', .opts = list(cite.style = "numeric"), after=" , pp. 19-20")`,  in an increasingly protectionist context — one that is hindering global trade — the T&T industry continues building bridges rather than walls between people, as made apparent by increasing numbers of people travelling across borders and
global trends toward adopting less restrictive visa policies. The share of ideas and cultures is a welcoming trend in the contemporary world. More and more, governments around the world are realizing that, for the most part, barriers to travel are not making people
and countries safer, but are hindering economic growth, job creation and tolerance between countries. With a growing “wanderlust”, there is a unique opportunity for many countries to benefit from the T&T industry while, at the same time, ensuring the security of borders and citizens. This trend is sustained by diverging underlying policies in trade and tourism. In 2016, destinations worldwide required 58% of the world’s population to obtain a visa prior to departure. In 2007 this number was 77%. 

Furthermore, a report by the World Bank `r Cite(bib, 'WB', .opts = list(cite.style = "numeric"))` concludes that tourism provides opportunities for economic diversification and skills upgrading. The tourism sector provides a means by which local entrepreneurs can experiment with new products and test them on international markets in their home country before exporting. International tourists typically create demand for products and services which may not have already existed in the local market and also demand certain quality standards. Whilst these can be a challenge to meet in the short-term, tourism creates the market and the incentive to drive the process – leading to growth and improvement over time. 

We can see that countries benefit from tourism flows both economically and socially. In this paper we want to find which economic and social variables significantly influence or have no effect on the tourism flows to help governments in deciding which policies to adapt in order to grow the tourism sector. 

For this paper only public and free data is used which is strongly aggregated. This could mask some significant variables. It is strongly believed that using not public data that has a very thorough structure of the tourist arrivals the task of finding influential variables to toursim flows would be more straightforward. Nonetheless, the data from the OECD and World Bank's databases have annual cross-sectional data for the countries mentioned above which enables us to use various econometric tools in order to find significant variables to help explain the tourism flows.  

\newpage

\section{Economic variable selection}

The increase in tourism is evident in the countries which we will analyze. To visualize this we will plot the overall sum of arrivals in every year.

```{r arrival trend, echo=F, fig.cap="Trend of tourists arrivals", fig.height=6, fig.width=8,  fig.pos="!ht", fig.align="center"}

dt.a <- master.data[, c("Country", "Date", "Total.Arrivals"), ]

dt.a <- ddply(dt.a, ~Date, function(xframe){
  
  xframe <<- xframe
  
  results <- data.frame(Country='All', Date=xframe$Date[1], Total.Arrivals=sum(xframe$Total.Arrivals, na.rm=T))
  xframe <- rbind(xframe, results)
  return(xframe[nrow(xframe), ])

})

dt.a$Total.Arrivals <- dt.a$Total.Arrivals/1000
dt.a$Date <- as.numeric(dt.a$Date)

p <- ggplot(dt.a, aes(x=Date, y=Total.Arrivals, group=Country, colour=Country, label=Country)) + geom_line(size=1.2) + ylab("Total Arrivals, thousands") + theme_bw() + scale_x_continuous(expand = c(0, 4)) + geom_dl(aes(label = Country), method = list(dl.combine("first.polygons"), cex = 1)) + geom_dl(aes(label = Country), method = list(dl.combine("last.polygons"), cex = 1)) + stat_smooth(se = FALSE, method = "lm", formula = y ~ ns(x,1),aes( colour="Linear trend"), linetype="dashed")  + scale_colour_manual(values = c("coral3", "darkblue"),
                       guide = guide_legend(override.aes = list(
                         linetype = c("solid", "dashed"),
                         shape = c(16, 16)), title="Legend"))

plot(p)

```

From figure 2 we can clearly see that the overal sum of tourist arrivals through the years is increasing rapidly. 

The aim of this paper is to create a model to help explain the total number of arrivals of tourists to a given OECD country. There are several factors influencing the tourism market in general `r Cite(bib=bib, 'factors', .opts = list(cite.style = "numeric"))`:

$\bullet$ Environmental factors 

$\bullet$ Historical and cultural factors

$\bullet$ Socio-economic factors 

$\bullet$ Religious factors 

$\bullet$ Other factors 

In this paper we will focus more on the socio-economic ones, because they in general can be expressed in numbers and are not constant over time. 

Accessibility is an important socio-economic variable. All tourist centers must be easily accessible by various modes of transportation like roads, railways, air and water. To reflect accessibility the investments to inroad infrastructure ($Investment$) variable is used.

Another big influencer to tourism are the various attraction sites in a given country: ranging from big ones like amusement parks, natural wonder sites to the small ones like coffehouses, restaurants. All of these businesses have to be maintained and a big factor are the taxes on these services. As a reflection of how governments help or not in increasing the tourism flows to these businesses  the taxes on goods and services ($tax$) variable is used. Also, the overall economic state of a country has a strong correlation with the amount of new shops, hotels and other attractions that can be opened in a given country. As a proxy for the overall state of a country GDP per capita ($GDP$) variable is used. Also, a better economic state of a country means that there are more ancillary services: banking and finance, internet and telecom connectivity, hospitals, insurance and so on. Adding to the socio-economic variables it can be argued that the more hours are worked in a country means more services that could be provided for the tourists. To measure this we will use the annual average hours worked variable ($hours worked$).  

Additional variable that may influence tourism in the Western countries may be terrorism. At least in the Islamic countries, in the country where the attack took place, one additional fatal incident is predicted to reduce the tourist flow from the country of the main victims by 4.2 per cent in the same year and by 7.4 per cent in the subsequent one `r Cite(bib=bib, 'terror.absolute', .opts = list(cite.style = "numeric"))`.

There is also the so called the spill-over effect:

If nationals from a certain country, the UK for example, have become victims of fatal terrorist incidents in an Islamic country, such as Tunisia for example, this also affects tourism from the UK to other Islamic countries, such as Egypt. Thus, one additional incident in which British citizens were the main victims in one Islamic destination is predicted to reduce the tourist flow from the UK to another Islamic country by 3.8 per cent in the same year and by 3.7 per cent in the next one `r Cite(bib=bib, 'terror.absolute', .opts = list(cite.style = "numeric"))`.  The terrorist attacks variable was taken from the Department of Homeland Security Center of Excellence led by the Maryland University's database and we will test the hypothesis whether percentage changes in terror attacks have a significant impact to percentage changes in tourist arrivals in the Western countries.

Taking into account all of this information we will try to find the function \textbf{f} in the equation:

\[
log(A)_{it}  = \textbf{f}(\Delta log(GDP)_{it}, \Delta terror_{it}, \Delta log(hours)_{it}, \Delta tax_{it}, \Delta Investment)
\]

Here 

$A_{it}$ - total number of arrivals to a country i at time period t, thousands

$GDP_{it}$ - GDP per capita at a country i at time period t, thousands 

$terror_{it}$ - total number of terrorist attacks at country i at time period t

$hours_{it}$ - average numbers of hours worked annualy in country i at time period t

$tax_{it}$ - percentage of taxes on goods and services.

$Investment_{it}$ - investment to inland infrastructure, percent of GDP.

<!-- \subsection{Motivation behind variable selection} -->

All of these variables are available for France, United Kingdom, USA, Canada, Germany, Spain, Portugal, Netherlands, Austria, Italy and Belgium from the year 1996 to 2014. Note that we will be using cross-sectional panel data for each of the countries. This equals to 209 observations.

\newpage

\subsection{Gross domestic product per capita}

The variable $GDP$ is chosen to reflect the economic situation and the standart of living in a given country. GDP per capita is calculated by taking the total output of a country and dividing it by the number of people in the country. The division by the number of people helps us to compare economies between countries. 

```{r OECD GDP per capita, echo=F, fig.cap="GDP per capita of OECD", fig.height=6, fig.width=8,  fig.pos="!ht", fig.align="center"}

dt.a <- master.data[, c("Country", "Date", "gdp")]

dt.a <- ddply(dt.a, ~Date, function(xframe){
  
  xframe <<- xframe
  
  results <- data.frame(Country='All', Date=xframe$Date[1], gdp=mean(xframe$gdp, na.rm=T))
  xframe <- rbind(xframe, results)
  return(xframe[nrow(xframe), ])

})

dt.a <- rename(dt.a, c("gdp" = "value"))
dt.a$Date <- as.numeric(dt.a$Date)

p <- ggplot(dt.a, aes(x=Date, y=value, group=Country, colour=Country, label=Country)) + geom_line(size=1.2) + ylab("GDP per capita, USD") + theme_bw() + scale_x_continuous(expand = c(0, 4)) + geom_dl(aes(label = Country), method = list(dl.combine("first.polygons"), cex = 1)) + geom_dl(aes(label = Country), method = list(dl.combine("last.polygons"), cex = 1))  + stat_smooth(se = FALSE, method = "lm", formula = y ~ x,aes( colour="Linear trend"), linetype="dashed") +
scale_colour_manual(values = c("darkblue", "coral3"),
                       guide = guide_legend(override.aes = list(
                         linetype = c("dashed", "solid"),
                         shape = c(16, 16)), title="Legend")) 

plot(p)

```


Ever since Simon Kuznets presented the idea of GDP (or rather GNP (gross national product) back in those days) to US congress in 1934 `r Cite(bib=bib, 'GDP', .opts = list(cite.style = "numeric"), after=" ,see pp. 6")` this number has been the dominant measure of economic growth. Altough it was invented to better gauge the economy of the USA in the Great Depression and help the government to implement the right policies, GDP as a measure has spreaded to almost all of the world as a basis for governments to keep track of the economies. GDP is defined by the following formula for country $i$ at time period $t$:

$$GDP_{it} = consumption_{it} + investment_{it} + government spending_{it} + exports_{it} - imports_{it}$$

```{r bvp vs tourist arrivals, echo=F, warning=F ,fig.cap="Percentage change in GDP vs percentage change in total arrivals", fig.height=6, fig.width=10,  fig.pos="!ht", fig.align="center"}

par(mfrow=c(1, 2))

grid.frame(x=pdata$gdp, y=pdata$Total.Arrivals)

for(cn in unique(pdata$Country)){
  matplot(x=pdata[pdata$Country==cn, "gdp"], y=pdata[pdata$Country==cn, "Total.Arrivals"], type="p", pch=19, xlab="GDP per capita", ylab="Total arrivals", add=T, col=which(cn==unique(pdata$Country)))
}

 matplot(x=pdata$gdp, y=pdata$Total.Arrivals, type="p", pch=19, xlab="GDP per capita", ylab="Total arrivals", col="dodgerblue1")
 grid()
 
```

As we can see from figure 4, visually there is no clear relationship between changes in GDP per capita and total number of tourist arrivals. The colours on the graph on the left simbolize a different country. In order to see a more clear picture we need to plot not all of the countries but some of them. Because of the panel structure of our data some visual trends might be lost because of the abundance of points. 

```{r bvp vs tourist arrivals selected countries, echo=F, warning=F ,fig.cap="Percentage change in GDP vs percentage change in total arrivals", fig.height=8, fig.width=9,  fig.pos="!ht", fig.align="center"}

 par(mfrow=c(2, 2))

for(cn in unique(pdata$Country)[1:4]){
  
  grid.frame(x=pdata$gdp, y=pdata$Total.Arrivals, xlab="GDP per capita", ylab="Total arrivals")
  matplot(x=pdata[pdata$Country==cn, "gdp"], y=pdata[pdata$Country==cn, "Total.Arrivals"], type="p", pch=19, add=T, col="blue")
  model <- lm(Total.Arrivals ~ gdp, data=pdata[pdata$Country==cn, ])
  abline(model, col="red")
  info <- summary(model)$coefficients
  p.val <- info[, "Pr(>|t|)"][2]
  legendary2("Regression line", col="red")
  mtext(cn, adj=0, line=2, cex=1.2, col="dodgerblue1")
  mtext(paste("p - value for regressor:", round(as.numeric(p.val), 3)), line=1, adj=0, col="salmon1")
  
}

 
```

Figure 5 gives a more clear picture. Percentage changes in GDP per capita is significant for Austria from the 4 countries that are visualized. Panel models will help to tell that if GDP per capita is a significant variable for the whole group of countries and not for certain ones.   

All of the scatterplots for the economic variable mentioned above and the ones mentioned at the start of the section is added to the section "Scatter diagrams".

\newpage

\section{Modelling}

\subsection{Panel data structure}

Panel data provides information on individual behaviour, both across individuals and over time - the data has cross-sectional and time-series dimensions. Panel data includes N individuals over T regular time periods. We can denote a panel data model similarly like a typical bivariate reggression model `r Cite(bib, 'econometric.methods', .opts = list(cite.style = "numeric"), after=" , pp. 390")`:

\[
\tag{1}
  y_{it} = \alpha_{i} + \beta X_{it} + u_{it}
\]

The difference from a typical reggression model is that there are the $i$ and the $t$ terms, where $i$ denotes the individual and $t$ denotes the time period. The construction of a panel model starts with appropriately 'stacking' the data set. Assuming that y is the dependant variable and X is the matrix of the explanatory variables we need to have the following structure  `r Cite(bib, 'econometric.methods', .opts = list(cite.style = "numeric"), after=" , pp. 388")`: 

\[
  Y = y_{i} = \begin{bmatrix}
         y_{1}    \\
         y_{2}    \\[0.3em]
         \vdots   \\[0.3em]
         y_{N}     
  \end{bmatrix} = \begin{bmatrix}
         e    \\
         0    \\[0.3em]
         \vdots   \\[0.3em]
         0     
  \end{bmatrix}  \alpha_{1} + \begin{bmatrix}
       0    \\
       e    \\[0.3em]
       \vdots   \\[0.3em]
       0    
\end{bmatrix}  \alpha_{2} + \cdots + \begin{bmatrix}
       0    \\
       0    \\[0.3em]
       \vdots   \\[0.3em]
       e     \\[0.3em]
\end{bmatrix}  \alpha_{N} + \begin{bmatrix}
       x_{1}    \\[0.3em]
       x_{2}    \\[0.3em]
       \vdots   \\[0.3em]
       x_{n}     \\[0.3em]
\end{bmatrix} \beta +   \begin{bmatrix}
       u_{1}    \\[0.3em]
       u_{2}    \\[0.3em]
       \vdots   \\[0.3em]
       u_{n}     \\[0.3em]
\end{bmatrix}
\]

Here

\[
y_{i} = \begin{bmatrix}
       y_{i1}    \\
       y_{i2}    \\[0.3em]
       \vdots   \\[0.3em]
       y_{iT}     \\[0.3em]
\end{bmatrix} x_{i} = \begin{bmatrix}
       x_{1i1} & x_{2i1} & \cdots & x_{Ki1}    \\
       x_{1i2} & x_{2i2} & \cdots & x_{Ki2}   \\[0.3em]
       \vdots & \vdots & \cdots & \vdots   \\[0.3em]
      x_{1iT} & x_{2iT} & \cdots & x_{KiT}      \\[0.3em]
\end{bmatrix}
\]

For example, the 'head' of the raw data which will be analysed looks like this:

```{r visualising data table, echo=F}

kable(head(pdata), format = "latex", row.names = F)
# kable(tail(pdata), format = "latex", row.names = F)

```


\subsection{Unit roots}

Before modelling we need to check whether there are unit roots in our data set. We will use the \textit{purtest()} function to check for this. This function implements the Maddala and Wu test.

```{r adf data, echo=F, warning=F, comment=NA}

Arrivals <- as.data.frame(split(pdata$Total.Arrivals, pdata$Country))

GDP <- as.data.frame(split(pdata$gdp, pdata$Country))

Hours <- as.data.frame(split(pdata$hours.worked, pdata$Country))

Attacks <- as.data.frame(split(pdata$Terror.attacks, pdata$Country))

Tax <- as.data.frame(split(pdata$tax, pdata$Country))

Inve <- as.data.frame(split(pdata$Investment, pdata$Country))
```


```{r adf, echo=F, warning=F, comment=NA}

result.table <- matrix(ncol=2, nrow=6) %>% as.data.frame()
names(result.table) <- c("Variable", "p-value")
result.table$Variable <- c("Arrivals", "GDP", "Hours", "Attacks", "Tax", "Investment")

t <- purtest(Arrivals, pmax = 4, exo = "intercept", test = "madwu")

result.table[1, "p-value"] <- t$statistic$p.value

t <- purtest(GDP, pmax = 4, exo = "intercept", test = "madwu")

result.table[2, "p-value"] <- t$statistic$p.value

t <- purtest(Hours, pmax = 4, exo = "intercept", test = "madwu")

result.table[3, "p-value"] <- t$statistic$p.value

t <- purtest(Attacks, pmax = 4, exo = "intercept", test = "madwu")

result.table[4, "p-value"] <- t$statistic$p.value

t <- purtest(Tax, pmax = 4, exo = "intercept", test = "madwu")

result.table[5, "p-value"] <- t$statistic$p.value

t <- purtest(Inve, pmax = 4, exo = "intercept", test = "madwu")

result.table[6, "p-value"] <- t$statistic$p.value


```

\begin{center}

```{r visualising data table 2, echo=F}

kable(result.table, format = "latex", row.names = F, digits = 5)

```

\end{center}

The alternative hypothesis for the test is that the data does not have a unit root. All values in the table are much lower than 0.05. According to the tests no variable has a unit root.  

\newpage

\subsection{Pooling estimator}

The first model which we will be examining will be the simple OLS model which ignores the panel data structure. The equation is then:

\[
y = \alpha + X \beta + \epsilon
\]

And the estimator is derived solving the equation:

\[
\widehat{\beta} = \left( X X^{T} \right)^{-1}X^{T}y
\]

```{r pooling panel model, include=FALSE, echo=F, warning=F}

model.ols <- lm(formula = Total.Arrivals ~ Terror.attacks + hours.worked + tax + gdp + Investment, data=pdata)
# pdata <- pdata[-which(cooks.distance(model.ols)>(4/length(cooks.distance(model.ols)))), ]
# summary(model.ols)
```

```{r xtable, results="asis", echo=F, warning=F}
tab <- xtable(summary(model.ols)$coef, digits=c(3, 3, 3, 3, 3), caption = "OLS model")
print(tab, type="latex", floating = T, only.contents = F, comment = F)
```

We can use this model to inspect the residuals and remove some outliers from our data set. The residuals can be inspected in figure 6. We will remove all the points that are above the blue line in the cook's distance plot.

```{r cooks distance initiale, echo=F, fig.cap="Residuals", fig.height=7, fig.width=7,  fig.pos="h"}

par(mfrow=c(2, 1))  
plot(model.ols, 4, col="salmon1", lwd=2)
abline(h=4/length(model.ols$residuals), col="dodgerblue1", lwd=2)
legendary2(col="dodgerblue1", "4/number of obs.")
grid()

c.dist <- cooks.distance(model.ols)
r <- stdres(model.ols)
plot(r, type="l", xlab="number of obs.", ylab="Residaul", main="Plot of standartized residuals", col="salmon1")
abline(h=3*sd(r), col="dodgerblue1", lwd=2)
abline(h=-3*sd(r), col="dodgerblue1", lwd=2)
legendary2(col="dodgerblue1", "3 sigmas")
grid()

pdata <- pdata[-which(cooks.distance(model.ols)>(4/length(cooks.distance(model.ols)))), ]
```

```{r pooling panel model without outliers, include=FALSE, echo=F, warning=F}

model.ols <- lm(formula = Total.Arrivals ~  hours.worked + tax, data=pdata)
# pdata <- pdata[-which(cooks.distance(model.ols)>(4/length(cooks.distance(model.ols)))), ]
# summary(model.ols)
```

```{r xtable 2, results="asis", echo=F, warning=F}
tab <- xtable(summary(model.ols)$coef, digits=c(3, 3, 3, 3, 3), caption = "OLS model without outliers")
print(tab, type="latex", floating = T, only.contents = F, comment = F)
```

Table 3 represents the final linear model without the outliers and the insignificant variables. The problem with this model is that while giving consistent estimators, the standart errors of those estimators will be understated. Also, OLS is not efficient compared to the generalized least-squares procedure  `r Cite(bib, 'econometric.methods', .opts = list(cite.style = "numeric"), after=" , pp. 391")`.

\newpage

\subsection{Fixed effects model}

In this section we will be creating a fixed effects panel data model  `r Cite(bib, 'econometric.methods', .opts = list(cite.style = "numeric"), after=" , pp. 395 - 399")`. Recall equation (1). The $\alpha_{i}$ is called the \textit{individual effect}.
If $\alpha_{i}$ does correlate with $X_{it}$ then the panel model can be reffered to as a fixed effects model. 

\subsubsection{Model assumptions}

Let as assume that we have n individuals and T observations for each individual. The fixed effects model has the following structure:

$$y_{it} = X_{it} \beta + \epsilon_{it} $$
$$\epsilon_{it} = \alpha_{i} + \eta_{it} $$

The assumptions for the errors are:

$$\mathbf{E}[\eta] = 0 ; E[\eta\eta^{\prime}] = \sigma^{2}_{\eta}I_{nT}$$

$$\mathbf{E}[\alpha_{i}\alpha_{j}] = 0, \forall i\neq j;  E[\alpha\alpha^{\prime}] = \sigma^{2}_{\alpha}$$

$$\mathbf{E}[\alpha_{i}\eta_{j}] = 0, ;  E[\alpha] = 0$$

All of the above expectations are conditional on $X$. In the fixed effects model case the $\alpha_{i}$ term is treated as an unknown parameter and has to be estimated. 

\subsubsection{Obtaining estimates}

To obtain the $\widehat{\beta_{FE}}$ we first "demean" the data. $\forall i$:

$$y_{it} - \overline{y_{i}} = \left(X_{it} -  \overline{X_{i}}\right) \beta + \left( \alpha_{i}  - \overline{\alpha_{i}}\right) + \left( \epsilon_{i}  - \overline{\epsilon_{i}}\right)$$

Now since $\overline{\alpha_{i}}$ is just $\alpha_{i}$ then the above equations simplifies to:

$$\dot{y} := y_{it} - \overline{y_{i}} = \left(X_{it} -  \overline{X_{i}}\right) \beta + \left( \epsilon_{i}  - \overline{\epsilon_{i}}\right) =: \dot{X} \beta_{FE} + \dot{\epsilon}$$

To obtain the estimate $\widehat{\beta_{FE}}$ we simply run an OLS procedure to the equation above. The estimator $\widehat{\beta_{FE}}$ is often called the within estimator. Any procedure that eliminates the term $\alpha_{i}$ can be called a $\textit{within transformation}$. To obtain the $alpha_{i}$ for every $i$ we simply match the intercept coordinates that is estimated by running the reggresion for the demeaned data with the way the data was stacked. 

\newpage

\subsubsection{Results}

```{r fixed effects model, include=FALSE, echo=F}

model.fixed <- plm(formula = Total.Arrivals ~ Terror.attacks + hours.worked + tax + gdp + Investment, data=pdata, model="within")
# summary(model.fixed)
```

```{r, results='asis', echo=F, warning=F}

tab <- xtable(summary(model.fixed)$coef, digits=c(3, 3, 3, 3, 3), caption = "Fixed effects model")
print(tab, type="latex", floating = T, only.contents = F, comment = F)

```

As we can see from table 3 the coefficients have a simmilar signs to them as in the OLS case. The only significant variable is the average hours worked variable. One percent increase would lead to an increase of about the same amount in tourist arrivals.

After dropping the insignificant variables we have:


```{r final models, include=FALSE, echo=F}

model.fixed <- plm(formula = Total.Arrivals ~ Terror.attacks + hours.worked + tax, data=pdata, model="within")
ols.model <- lm(formula = Total.Arrivals ~ Terror.attacks + hours.worked + tax, data=pdata)
```

```{r, results='asis', echo=F, warning=F}

tab <- xtable(summary(model.fixed)$coef, digits=c(4, 4, 4, 4, 4), caption = "Final fixed effects model")
print(tab, type="latex", floating = T, only.contents = F, comment = F)

```

The final panel fixed effects model is presented in table 4. We will leave the variable regarding terror attacks altough it has a high p value (>0.1) because it is much lower than in the typical OLS case (table 2). According to this model, one additional terror attack would lead to a decrease of tourist arrivals by 0.01 percent. A one percent increase to annual hours worked would lead to a 1.02 percent increase of tourist arrivals and one percent increase to taxes on goods and services would lead to a decrease of about 0.9 percent of the tourists. 

Applying the F test for individual effects gives the result:

```{r pFtest, echo=T, comment=NA}
  pFtest(model.fixed, ols.model)
```

According to these results the fixed effects model is not significantly better in explaining the number of arrivals compared to the OLS model.

```{r estimating ols and fixed model fits, echo=F, comment=NA}
  
  pdata <- ddply(pdata, ~Country + Date, function(xframe){
    
    xframe <<- xframe
    
    
    ## fixed effects
    
    intercept <- fixef(model.fixed)[xframe$Country[1]] %>% as.numeric()
    X <- xframe[, c('Terror.attacks', 'hours.worked', "tax")]
    fit <- model.fixed$coefficients[1] * X[1] +  model.fixed$coefficients[2] * X[2] +  model.fixed$coefficients[3] * X[3] + intercept
    fit <- as.numeric(fit) # this is the fit of the logged number of arrivals
    xframe$fc.fixed.effect <- fit
    
    ## OLS
    
    X <- xframe[, c('hours.worked', "tax")] 
    fit <- model.ols$coefficients[2] * X[1] + model.ols$coefficients[3] * X[2]
    fit <- fit + model.ols$coefficients[1] 
    xframe$fc.OLS <- as.numeric(fit)
    
    return(xframe)
  })

```


```{r visualizing the differences between fits, echo=F, fig.cap="OLS and fixed effects model comparison", fig.height=6, fig.width=8,  fig.pos="!ht", fig.align="center"}
  
fdt <- pdata[, c("Country", "Date", "Total.Arrivals", "fc.fixed.effect", 'fc.OLS')]
# fdt$Total.Arrivals  <- exp(fdt$Total.Arrivals)/1000
# fdt$fc.fixed.effect <- exp(fdt$fc.fixed.effect)/1000
# fdt$fc.OLS          <- exp(fdt$fc.OLS)/1000
fdt$Date <- fdt$Date %>% as.character() %>% as.numeric()

## we will plot 4 countries in order to have a clear picture
cn <- c( "United States",  "Austria", "France", "Spain")
points <- data.frame()

par(mfrow=c(2,2))
for(cc in cn){
  
  tmp <- fdt[fdt$Country==cc, ]
  points <- tmp[, c("Total.Arrivals", "fc.fixed.effect", 'fc.OLS')]
  
grid.frame(x=as.numeric(tmp$Date), y=points)
  matplot(x=as.numeric(tmp$Date), y=points, lwd=1, lty=1, cex=1.25, pch=20, xlab="Time", ylab="Arrivals, thousands", add = T, type="o",
        col=c('salmon1', "firebrick1", 'cornflowerblue'))
  legendary2(c("Original", "Fixed effect fit", "OLS fit"), col=c('salmon1', "firebrick1", 'cornflowerblue'))
  mtext(cc, col="blueviolet", line=2, cex=1.25, adj = 0)
}

```

\newpage

\subsection{Statistics for accuracy}

We can measure the goodness of fit using several statistics. We will use the the \textit{mean absolute error} (MAE), \textit{mean squared error} (MSE)
and the \textit{mean absolute percentage error} (MAPE)  `r Cite(bib, 'accuracy', .opts = list(cite.style = "numeric"))`. These statistics are defined by:

\[
MSE = \dfrac{1}{n}  \sum_{i=1}^{n}\left( y - \widehat{y} \right)^{2}
\]

\[
MAE = \dfrac{1}{n}  \sum_{i=1}^{n}\left| y - \widehat{y} \right|
\]

\[
MAPE = \dfrac{100}{n}  \sum_{i=1}^{n}\left| \dfrac{y - \widehat{y}}{y} \right| 
\]

Here

$\widehat{y}$ - fitted value

$y$ - original value


$n$ - number of observations


\begin{center}

```{r forecast accuracy, echo=F, results= "asis", fig.pos="H"}
  
fdt <- pdata[, c("Country", "Date", "Total.Arrivals", "fc.fixed.effect", 'fc.OLS')]
# fdt$Total.Arrivals  <- exp(fdt$Total.Arrivals)
# fdt$fc.fixed.effect <- exp(fdt$fc.fixed.effect)
# fdt$fc.OLS          <- exp(fdt$fc.OLS)

accuracy.table <- matrix(ncol=2, nrow=3) %>% as.data.frame()
colnames(accuracy.table) <- c("OLS", "Fixed effects")
rownames(accuracy.table) <- c("MSE", "MAPE", "MAE")

## MSE:

accuracy.table[1, 1] <- MSE(fdt$Total.Arrivals, fdt$fc.OLS)
accuracy.table[1, 2] <- MSE(fdt$Total.Arrivals, fdt$fc.fixed.effect)

## MAPE

accuracy.table[2, 1] <- MAPE(fdt$Total.Arrivals, fdt$fc.OLS)
accuracy.table[2, 2] <- MAPE(fdt$Total.Arrivals, fdt$fc.fixed.effect)

## MAE

accuracy.table[3, 1] <- MAE(fdt$Total.Arrivals, fdt$fc.OLS)
accuracy.table[3, 2] <- MAE(fdt$Total.Arrivals, fdt$fc.fixed.effect)

# options("scipen" = 5)

accuracy.table <- apply(accuracy.table, c(1, 2), function(x) {
  # x <- format(x,scientific=FALSE)
  x <- as.numeric(x)
  x <- round(x, 3)
  return(x)
  })

tab <- xtable(accuracy.table[c(1,3), ], digits=c(3, 3, 3), caption = "Accuracy statistics results")
print(tab, type="latex", floating = T, only.contents = F, comment = F)
```

\end{center}

As we can see from table 5 and the statistics for accuracy table the fixed effects models' predictions are closer to the original values. Note that we have dropped the MAPE statistic because we are measuring percentage changes and the statistic would give misleading results.

\newpage

\subsection{Random effects model}

There are two main types of panel models: the fixed effects model and the random effects models `r Cite(bib, 'econometric.methods', .opts = list(cite.style = "numeric"), after=" , pp. 391 - 393")`. The main difference between them is the assumption of $\alpha_{i}$ regarding the regressor matrix $X_{it}$. In the random effects model case it is assumed that $\alpha_{i}$ and $X_{it}$ do not correlate.

```{r random effects model, include=FALSE, echo=F}

model.random <- plm(formula = Total.Arrivals ~ Terror.attacks + gdp + tax + hours.worked + Investment, data=pdata, model="random")
# summary(model.random)
```
```{r, results='asis', echo=F, warning=F}
  # texreg(model.random, caption = "Random effects model", custom.coef.names = c("intercept", "attacks", "log GDP", "log Hours"), digits = 3)

tab <- xtable(summary(model.random)$coef, digits=c(3, 3, 3, 3, 3), caption = "Random effects model")
print(tab, type="latex", floating = T, only.contents = F, comment = F)
```

Comparing tables 3 and 6 we can see that the signs near the coefficients are the same and the actual values of the estimators are almost even. The random effects model has a little higher R statistic. The main difference here is that there is the intercept term and we lose some "uniqueness" to the countries which was present with the fixed effects model. One measure to determine which model is better for the given data is the Hausman test. Just like in the previous cases, the only significant variable is the hours worked variable. The final random effects model:


```{r random effects model 2, include=FALSE, echo=F}

model.random <- plm(formula = Total.Arrivals ~ tax + hours.worked, data=pdata, model="random")

```
```{r, results='asis', echo=F, warning=F}

tab <- xtable(summary(model.random)$coef, digits=c(3, 3, 3, 3, 3), caption = "Random effects model")
print(tab, type="latex", floating = T, only.contents = F, comment = F)
```


```{r pHtest, echo=T, comment=NA}
  phtest(model.random, model.fixed)
```

As we can see from the test result we cannot reject the null hypothesis therefore we conclude that the random effects coefficients are more efficient and are consistent. Therefore the test implies that we sould use the random effects model.

\newpage

\subsection{Residuals}

As we saw in the previous section the Hausman test for panel data indicated that we should use the random effects model. The test is only one of the criteria for deciding which model to use. Inspection of the models' residuals is also needed.

```{r residuals of models, echo=F, fig.cap="Residuals of models", fig.height=6, fig.width=8,  fig.pos="!ht", fig.align="center"}
  
par(mfrow=c(1,2))

res.random <- residuals(model.random)
res.fixed  <- residuals(model.fixed)
  
grid.frame(x=seq(from=1, length.out = length(res.random)), y=as.numeric(res.random))
matplot(x=seq(from=1, length.out = length(res.random)), y=as.numeric(res.random), lwd=1, lty=1, cex=1.25, pch=20, ylab="Residual value", xlab="Index", type="o", add=T, col=c('salmon1'))
mtext("Random effects model residuals", col="salmon1", line=1, cex=1.25, adj = 0)

grid.frame(x=seq(from=1, length.out = length(res.random)), y=as.numeric(res.random))
matplot(x=seq(from=1, length.out = length(res.fixed)), y=as.numeric(res.fixed), lwd=1, lty=1, cex=1.25, pch=20, ylab="Residual value", xlab="Index", type="o", col=c('cornflowerblue'), add=T)
mtext("Fixed effects model residuals", col="cornflowerblue", line=1, cex=1.25, adj = 0)

```

As we can see in figure 8 the residuals visually look very similar. 

```{r distribution of residuals, echo=F, fig.cap="Distribution of the residuals", fig.height=6, fig.width=8,  fig.pos="!ht", fig.align="center"}
  
par(mfrow=c(2,2))

res.random <- residuals(model.random)
res.fixed  <- residuals(model.fixed)
  
hist(res.random, col = "cornsilk3", xlab = "Residuals of the random model", main="Density of the residuals")
hist(res.fixed, col = "cornsilk3", xlab = "Residuals of the fixed effects model", main="Density of the residuals")

qqnorm(res.random, ylab='Residuals', main = "Q - Q plot for random effects model", col="cornflowerblue")
grid()
qqline(res.random)

qqnorm(res.fixed, ylab='Residuals', main = "Q - Q plot for fixed - effects model", col="cornflowerblue")
grid()
qqline(res.fixed)

```

Figure 9 shows the histogram and the quantile to quantile plot of the residuals. In both cases, the residuals appear to follow a bell - shaped curve which implies normality. Altough the q q plots show that the residuals have heavy tails. This can be expected because real life data very often is not 'clean' enough to give normally distributed errors. 

Both the random effects models and fixed effects models are not satisfactory because of the residuals, low R - square and the fitted values. Additionally, the random effect models' underlying assumption of $\alpha_{i}$ not beeing correlated with the explanatory variables is often not true. In the next section we will switch to a dynamic panel model.

\newpage

\subsection{Dynamic panel data model}


In this section we will be creating a dynamic panel model (we will include lags of the variables). First of all, the estimation of the coefficient $\delta$ in the equation:

$$ y_{it} = \delta y_{it - 1} + \beta X_{it} + \alpha_{i} + \epsilon_{it} $$

is not as straightforward as just adding another collumn to the regressor matrix with the lagged variable $y_{it}$. This is because the OLS estimate will be biased. This is because the $y_{it}$ is a function $\epsilon_{it}$ and thus so is $y_{it-1}$ giving us the multicolinearity problem. 

Consider the simple model:

$$y_{it} = \delta y_{it - 1}+ \alpha_{i} + \epsilon_{it}$$

Taking the first differences we get rid of the individual effecct $\alpha_{i}$:

$$\Delta y_{it} = y_{it} - y_{it - 1} = \delta y_{it-1} + \alpha_{i} + \epsilon_{it} - \delta y_{it - 2} - \alpha_{i} - \epsilon_{it-1} = \delta (y_{it-1} - y_{it-2}) + \epsilon_{it} - \epsilon_{it-1} $$
The classic method to deal with multicolinearity is to find an instrumental variable that correlates with the endogenous variable and not with the residuals. Starting from time period t = 3:

$$\Delta y_{i3} = y_{i3} - y_{i2} = \delta (y_{i2} - y_{i1}) + (\epsilon_{i3} - \epsilon_{it-2}) $$
Here we can see that the variable $y_{i1}$ does not correlate with $(\epsilon_{i3} - \epsilon_{i2})$. 

When t = 4:

$$\Delta y_{i4} = y_{i4} - y_{i3} = \delta (y_{i3} - y_{i2}) + (\epsilon_{i4} - \epsilon_{i3}) $$
Both the $y_{i1}$ and ${y_{i2}}$ are good options. Going forward up to time period T we have the instrument vector:

$$(y_{i1}, y_{i2}, ..., y_{iT- 2})$$. If there are strictly exogenous variables in our system $x_{it}$ that correlate with the endogenous variable and not with the residuals they should also be included in the weight matrix diagonals (for a detailed proof `r Cite(bib, 'dynamic.panel', .opts = list(cite.style = "numeric"))`):


$$W_{i} = \begin{pmatrix}
    [y_{i1}, x_{i1}, x_{i2}]       & 0 & \dots & 0 \\
    0       &  [y_{i1}, y_{i2} x_{i1}, x_{i2}, x_{i3}] & \dots & 0 \\
    0      & 0  & \dots & [y_{i1}, y_{i2}, ..., y_{iT-2}, x_{i1}, x_{i2}, x_{i3}, ..., x_{iT-1}]
\end{pmatrix} $$

Then the consistent Arriano-Bond estimator is given by:

$$\widehat{\delta} =  \left[ (\Delta y_{t - 1})^{t} W (\widehat{V}_{W})^{-1} W^{t} \Delta y_{t - 1}\right]^{-1} x \left[ (\Delta y_{t - 1})^{t} W (\widehat{V}_{W})^{-1} W^{t} \Delta y_{t - 1}\right] $$
$$ \widehat{V}_{W} = \sum_{i=1}^{n} W_{i}^{t} \Delta \epsilon_{i} \Delta \epsilon_{i}^{t}  W_{i}$$

```{r dynamic model, include=FALSE, echo=F}

model.dynamic <- suppressWarnings( pgmm(Total.Arrivals ~ Terror.attacks + hours.worked + tax,
                       lag.form=list(1, 0, 0, 0), gmm.inst = ~ Terror.attacks + hours.worked + tax + gdp + Investment, 
                       lag.gmm = list(c(2,99)),
                       effect="individual",
                       data=pdata))
```
```{r, results='asis', echo=F, warning=F, fig.pos="H"}
  # texreg(model.train, caption = "Fixed effects model with training set", custom.coef.names = c("attacks", "log GDP", "log Hours"), digits = 3)

tab <- xtable(summary(model.dynamic)$coef, digits=c(4, 4, 4, 4, 4), caption = "Dynamic panel data model")
print(tab, type="latex", floating = T, only.contents = F, comment = F)
```

After trying various lags and taking into account the economic logic behind the model we get the results in table 8. The dynamic model shows that terror attacks has a significant influence to tourist arrivals. One additional terror attack would decrease the total number of tourist arrivals by 0.02 percent. The hours worked and the tax variables are also significant with a simillar absolute value as in the fixed effects or the random effects case. An additional significant variable is the lag of the changes in total arrivals. A positive increase in the lags of tourist arrivals will have a positive change to the present change of tourist arrivals. 

\newpage

\subsection{Dynamic models' residuals}

```{r residuals of dynamic models, echo=F, fig.cap="Residuals of models", fig.height=9, fig.width=8,  fig.pos="!ht", fig.align="center"}
  
par(mfrow=c(3, 1))

res.dynamic <- model.dynamic$residuals %>% unlist
  
grid.frame(x=seq(from=1, length.out = length(res.dynamic)), y=as.numeric(res.dynamic))
matplot(x=seq(from=1, length.out = length(res.dynamic)), y=as.numeric(res.dynamic), lwd=1, lty=1, cex=1.25, pch=20, ylab="Residual value", xlab="Index", type="o", add=T, col=c('salmon1'))
mtext("Dynamic model residuals", col="salmon1", line=1, cex=1.25, adj = 0)

hist(res.dynamic, col = "cornsilk3", xlab = "Residuals of the dynamic model", main="Density of the residuals")

qqnorm(res.fixed, ylab='Residuals', main = "Q - Q plot for dynamic model", col="cornflowerblue")
grid()
qqline(res.fixed)

```
As we can see in figure 10 the residuals resemble a simmilar pattern as in the case with static panel models. Altough the quantile to quantile plot shows "lighter" tails than in the previous cases. 

\newpage

\subsection{Conclusion of panel data models' analysis}

Based on the data and using static random effects ant fixed effects panel models we can conlude that the only significant variables to tourist arrivals' changes is changes to the average hours worked in a given country and the changes to taxes on goods and services. This can be explained that more hours worked and lower taxes would lead to more bussinesses and hence an increase in tourist attractions in a given country. On average, a one percent increase in hours worked would lead to an increase of 1 percent of tourist arrivals and a one percent increase in taxes to goods and services would decline the number of tourist arrivals by about 0.9 percent. Additionally, terrorist attacks have weak influence to the rate of change of arrivals. One attack would lead to a decrease of about 0.01 of tourists. It would take about 100 additional attacks a year to lover tourist arrivals by 1 percent. Thus its influence is not very high. This can be explainded that the services that prevent terrorism is doing a good job and the relative number of incidents are not that high to prevent tourists from visiting a country.

According to the dynamic model, the lag of one of the rate of change of tourist arrivals is also a significant variable which correlates positivelly with the changes in the present to tourist arrivals. 

\newpage

\subsection{Structure models}

In this section we will be creating a model which uses the structure data that can be obtained from the OECD database. Structure in this section means that there is information not only about total tourist arrivals but total arrivals from certain countries. For example, the United Kingdom's data for the year 2014:

```{r visualising structure data, echo=F}

kable(struct[struct$Country=="United Kingdom" & struct$Date=="2014", ], format = "latex", row.names = F)

```

The downside is that we lose the cross-sectional panel structure of our data and that there are only available years from 2009 to 2014 (after differentiation). The matrices for linear modelling will be $Y$ and $X$ where:

$Y_{ikt}$ - the total number of arrivals from country k to country i at time period t.

$X_{itk}$ - both the domestic and abroad economy's GDP per capita, domestic terror attacks, domestic taxes on goods and services, domestic investment to infrastructure.

We will be estimating the $\beta$ in:

$$Y_{itk} = \alpha + \beta X_{itk} + \epsilon$$

We will use the OLS procedure:

$$ \widehat{\beta} = \left( X^{T} X \right)^{-1}X^{T}Y $$


```{r structured panel data, echo=F, include=FALSE}

sdt <- struct
sdt <- sdt[!(sdt$Indicator %in% c("Other collective establishments", "Hotels and similar establishments", "Overnight visitors (tourists)", "Total international arrivals", "Same-day visitors (excursionists)", "Nights in all types of accommodation", 
                                  "Specialised establishments", "Private accommodation")), ]

sdt <- sdt[sdt$Country %in% unique(pdata$Country) & sdt$Indicator %in% unique(gdp$Country), ] %>% rename(c("Value" = "Arrivals", "Indicator" = "Origin"))

sdt.sub <- sdt
sdt.sub <- ddply(sdt.sub, ~Country + Origin, function(xframe){
  
  xframe <<- xframe
  if(dim(xframe)[1] >= 6){
    
    ## Adding variable for domestic economy
    
    xframe <- merge(xframe, gdp) %>% rename(c("value" = "gdp.d"))
    xframe$Indicator <- NULL
    
    ## Adding variable for foreign economy
    
    gdp.a <- gdp %>% filter(Country==xframe$Origin[1], Date %in% 2008:2014) %>% rename(c("value" = "gdp.a"))
    xframe$gdp.a <- gdp.a[, "gdp.a"]
    
    ## Adding hours worked 

    h <- hours %>% filter(Country==xframe$Country[1], Date %in% 2008:2014) %>% rename(c("Value" = "hours.worked"))
    xframe$hours <- h[, "hours.worked"]

    ## Adding terror attacks
    
    ter <- terror %>% filter(Country==xframe$Country[1], Date %in% 2008:2014)
    xframe$terror <- ter[, "Terror.attacks"]
    
    ## Adding taxes
    
    taxes <- tax %>% filter(Country==xframe$Country[1], Date %in% 2008:2014) %>% rename(c("Value" = "taxes"))
    xframe$taxes <- taxes[, "taxes"]
    
   ## Adding investments
    
    Invest <- Inv %>% filter(Country==xframe$Country[1], Date %in% 2008:2014) %>% rename(c("Value" = "Investment"))
    xframe$Investment <- Invest[, "Investment"]
    
    
    xframe$hours <- c(NA, diff(log(xframe$hours)))
    xframe$gdp.d <- c(NA, diff(log(xframe$gdp.d)))
    xframe$gdp.a <- c(NA, diff(log(xframe$gdp.a)))
    xframe$Arrivals <- c(NA, diff(log(xframe$Arrivals))) 
    xframe$terror <- c(NA, diff(xframe$terror)) 
    xframe$taxes  <- c(NA, diff(xframe$taxes))
    xframe$Investment  <- c(NA, diff(xframe$Investment))
    
  }
  
    return(xframe)
})

sdt.sub$Origin <- NULL
sdt.sub <- sdt.sub[complete.cases(sdt.sub), ]

model.ols <-  lm(Arrivals ~  gdp.a + gdp.d + terror + hours + taxes + Investment, data=sdt.sub)

```

The structure of Y is (only the first 5 rows of Y):


```{r visualising structure data Y, echo=F}

kable(sdt.sub[1:5, c(1:3)])

```

The $Country$ and $Date$ are added just for clarification.

The structure of X is (only the first 5 rows of X):

```{r visualising structure data X, echo=F}

dttt <- cbind(rep.int(1, 5), sdt.sub[1:5, c(4:9)])
names(dttt)[1] <- "Intercept"
kable(dttt)

```
  
The X is a matrix of 7 collumns (intercept collumn and the economic variables) and has the same number of rows as Y.

```{r, results='asis', echo=F, warning=F}

tab <- xtable(summary(model.ols)$coef, digits=c(3, 3, 3, 3, 3), caption = "OLS model")
print(tab, type="latex", floating = T, only.contents = F, comment = F)
```

As we can see from table 11, the only significant variables are GDP per capita abroad and hours worked domestically (with significance level of 0.1).

```{r, results='asis', echo=F, warning=F}

model.ols <-  lm(Arrivals ~  gdp.a +  hours, data=sdt.sub)
tab <- xtable(summary(model.ols)$coef, digits=c(3, 3, 3, 3, 3), caption = "Final OLS model")
print(tab, type="latex", floating = T, only.contents = F, comment = F)

```

As in the case of panel models, the terror attack variable is not significant. What is more interesting that the number of tourist arrivals does not depend on the local economy but rather on the economic states of countries from which there are visitors. A one percent change in GDP of a country from which tourists are arriving leads to an increase of 0.343 percent in positive change to tourist arrivals.

\newpage

\subsection{Residual analysis}

```{r distribution of residuals of final model, echo=F, fig.cap="Distribution of the residuals", fig.height=7, fig.width=8,  fig.pos="h"}
  
par(mfrow=c(2,1))

res <- residuals(model.ols)

hist(res, col = "cornsilk3", xlab = "Residuals of the structure model", main="Density of the residuals")

qqnorm(res, ylab='Residuals', main = "Q - Q plot for structure model", col="cornflowerblue")
grid()
qqline(res)

```

As we can see from figure 11 we can conclude that the residuals from the structure model graphically look the most normal out of all the models in this analysis so far.

```{r cooks distance, echo=F, fig.cap="Distribution of the residuals", fig.height=7, fig.width=7,  fig.pos="h"}

par(mfrow=c(2, 1))  
plot(model.ols, 4, col="salmon1", lwd=2)
abline(h=4/nrow(sdt.sub), col="dodgerblue1", lwd=2)
legendary2(col="dodgerblue1", "4/number of obs.")
grid()

c.dist <- cooks.distance(model.ols)
r <- stdres(model.ols)
plot(r, type="l", xlab="number of obs.", ylab="Residaul", main="Plot of standartized residuals", col="salmon1")
abline(h=3*sd(r), col="dodgerblue1", lwd=2)
abline(h=-3*sd(r), col="dodgerblue1", lwd=2)
legendary2(col="dodgerblue1", "3 sigmas")
grid()

```

We can clearly see from figure 12 that there are some observations which cook's distance is much larger than the generally accepted rule of 4 divided by the number of observations. We will eliminate the points from our data set who's residual is larger in absolute value than 2.5.


```{r model without influantial points, results='asis', echo=F, warning=F}
  
sdt.sub$res <- abs(r)
sdt.sub <- sdt.sub[sdt.sub$res<2.5, ]
model.ols <- lm(Arrivals ~  gdp.a + gdp.d + terror + hours + taxes + Investment, data=sdt.sub)
tab <- xtable(summary(model.ols)$coef, digits=c(3, 3, 3, 3, 3), caption = "OLS model without the influential points")
print(tab, type="latex", floating = T, only.contents = F, comment = F)

```

Eliminating the influential points and re-regressed the model we got that the variable hours is not significant after all. Also we can see that from figure 13 that the residuals are visually normal. The Shapiro-Wilk test's statistic is 0.039 which with alpha level of 0.01 would imply normality.

```{r distribution of residuals of final model 2, echo=F, fig.cap="Distribution of the residuals without influential points", fig.height=7, fig.width=7,  fig.pos="h"}

model.ols <-  lm(Arrivals ~  gdp.a, data=sdt.sub)    
par(mfrow=c(2,1))

res <- residuals(model.ols)

hist(res, col = "cornsilk3", xlab = "Residuals of the structure model", main="Density of the residuals")

qqnorm(res, ylab='Residuals', main = "Q - Q plot for structure model", col="cornflowerblue")
grid()
qqline(res)

```

```{r, results='asis', echo=F, warning=F}

model.ols <-  lm(Arrivals ~  gdp.a, data=sdt.sub)
tab <- xtable(summary(model.ols)$coef, digits=c(3, 3, 3, 3, 3), caption = "Final OLS model without influential points")
print(tab, type="latex", floating = T, only.contents = F, comment = F)

```

The final structure model is in table 14.

\newpage

\section{All main models}


In this thesis 3 main models with cross-sectional panel data were created:

```{r all models, results='asis', echo=F, warning=F}

tab <- xtable(summary(model.fixed)$coef, digits=c(4, 4, 4, 4, 4), caption = "Final fixed effects model")
print(tab, type="latex", floating = T, only.contents = F, comment = F)

tab <- xtable(summary(model.random)$coef, digits=c(4, 4, 4, 4, 4), caption = "Final random effects model")
print(tab, type="latex", floating = T, only.contents = F, comment = F)

tab <- xtable(suppressWarnings(summary(model.dynamic)$coef), digits=c(4, 4, 4, 4, 4), caption = "Final dynamic panel data model")
print(tab, type="latex", floating = T, only.contents = F, comment = F)


```

And the model with structure data:

\centering

```{r all models addition, results='asis', echo=F, warning=F}

tab <- xtable(summary(model.ols)$coef, digits=c(4, 4, 4, 4, 4), caption = "Final OLS model with structure data")
print(tab, type="latex", floating = F, only.contents = F, comment = F)

```

\raggedright

\newpage

\section{Conclusion}

Based on random effects, fixed effects, dynamic panel models and linear models:

$\bullet$ Increasing GDP per capita in a given country does not influence tourist arrivals to that country.

$\bullet$ Percentage change of taxes on goods and services in country i influence the rate of change in tourist arrivals. A 1 percent increase to taxes on goods and services would lead to a decline of about 0.9 percent in tourist arrivals. 

$\bullet$ Percentage change of investments to infrastructure in country i influence the rate of change in tourist arrivals. But the sign near the coefficient was negative in all of the models which would imply that a positive change in investments would lead to a decline in tourist arrivals. Hence we dropped this variable from our final model. 

$\bullet$ Percentage change in annual hours worked has a significant influence to the percentage change of tourist arrivals. A one percent increase in hours worked annually would increase the tourism flows by about 1.02 percent.  

$\bullet$ The dynamic panel model showed that the lag of one of the rate of change in tourist arrivals has a significant impact to the current rate of tourist arrivals. A one percent increase in tourist arrivals would result in an additional growth of about 0.07 percent of tourist arrivals in the future.  

Based on the structure data from the OECD data base:

$\bullet$ As in the panel models case, the domestic GDP per capita changes had no significant effect to the percentage changes in tourist arrivals.

$\bullet$ The most significant variable regarding the percentage change in tourist arrivals to country i is the percentage change in GDP per capita in abroad countries from whom the tourists come to the country i. A one percent increase in foreign economies leads to about 0.39 percentage increase in tourist arrivals.


\newpage

\section{Tests}

In this paper there were implemented two tests in order to inspect the panel models: the F test for individual effects and the Haussman test to determine which model - random or fixed effects - is more suitable. 

\subsection{F test}

The f test `r Cite(bib, 'F.test', .opts = list(cite.style = "numeric"), after=" , pp. 3-4")` that is used with the panel models in R statistical package tests the following hypothesis: assume that we have two models: 

restricted
$$y_{it} = \alpha + \beta X_{it} + \epsilon_{it}$$
and unrestricted:

$$y_{it} = \alpha + u_{i} + \beta X_{it} + \epsilon_{it}$$

Then the hypothesis is 


\[\begin{array}{lr}
        H_{0}: u_{i} =0,  \forall i \\
        H_{1}: \exists i:    u_{i} \neq 0,  
        \end{array}\]

The alternative hypothesis would mean that there exists a country where adding the term $u_{i}$ the r-squared statistic increases. 

To test this hypothes we use the F statistic (hence the name of the test):

$$F = \dfrac{(ESS_{R} - ESS_{U})/(N- 1) }{ESS_{U}/((T-1)N- K)}$$

Here 

$ESS_{U}$ - error sum of squares of the unrestricted model

$ESS_{R}$ - error sum of squares of the restricted model

$N$ - number of countries

$T$ - number of time periods

Under the assumption that the residuals are gaussian then the F statistic is distributed by the $\textbf{F}$ distribution with N-1, N(T-1) - K degrees of freedom.

\newpage

\subsection{Hausman test for panel models}

The Hausman test `r Cite(bib, 'Hausman.test', .opts = list(cite.style = "numeric"), after=" , pp. 39-40")` is used to determine whether $X_{it}$ are uncorrelated with $\alpha_{i}$. To test this we denote two estimators:

$\widehat{\beta_{FE}}$ - fixed effects estimate. 

$\widehat{\beta_{RE}}$ - random effects estimate.

The hypothesis is then:

 Estimator                $H_{0}$ holds              $H_{0}$ is rejected  
-------                  -------------              -----------------
$\widehat{\beta_{RE}}$    consistent;                 Inconsistent
                          efficient
$\widehat{\beta_{FE}}$    consistent;                  consistent       
                          inefficient

-------                  -------------              -----------------


Since under null hypothesis $\widehat{\beta_{RE}}$ is efficient then 

$$Var(\widehat{\beta_{FE}} - \widehat{\beta_{RE}}) =Var(\widehat{\beta_{FE}}) - Var(\widehat{\beta_{RE}})$$
Then the Hausman statistic is defined as:

$$H = \left( \widehat{\beta_{FE}} - \widehat{\beta_{RE}} \right) ^{T} \left(Var(\widehat{\beta_{FE}}) - Var(\widehat{\beta_{RE}})\right) \left( \widehat{\beta_{FE}} - \widehat{\beta_{RE}} \right)$$

and 

$$H \overset{asy}{\sim} \chi^2$$

The main idea of this test is that if $Var(\widehat{\beta_{FE}})$ is smaller than  $Var(\widehat{\beta_{RE}})$ it means that the H statistic will be "large" and the null hypothesis will be rejected. This in turn means that $\widehat{\beta_{RE}}$ is no efficient because there exists another statistic with a smaller variance.

\newpage

\section{Scatter diagrams}

```{r,echo=F, fig.cap="GDP per capita vs total arrivals", fig.height=9, fig.width=8,  fig.pos="!ht", fig.align="center"}
 
par(mfrow=c(4, 3))

for(cn in unique(pdata$Country)){
  
  grid.frame(x=pdata$gdp, y=pdata$Total.Arrivals, xlab="GDP per capita", ylab="Total arrivals")
  matplot(x=pdata[pdata$Country==cn, "gdp"], y=pdata[pdata$Country==cn, "Total.Arrivals"], type="p", pch=19, add=T, col="blue")
  model <- lm(Total.Arrivals ~ gdp, data=pdata[pdata$Country==cn, ])
  abline(model, col="red")
  info <- summary(model)$coefficients
  p.val <- info[, "Pr(>|t|)"][2]
  legendary2("Regression line", col="red")
  mtext(cn, adj=0, line=2, cex=1.2, col="dodgerblue1")
  mtext(paste("p - value for regressor:", round(as.numeric(p.val), 3)), line=1, adj=0, col="salmon1")
  
}


```


```{r , echo=F, fig.cap="Hours worked vs total arrivals", fig.height=10, fig.width=8,  fig.pos="!ht", fig.align="center"}
 
par(mfrow=c(4, 3))

for(cn in unique(pdata$Country)){
  
  grid.frame(x=pdata$hours.worked, y=pdata$Total.Arrivals, xlab="Hours worked", ylab="Total arrivals")
  matplot(x=pdata[pdata$Country==cn, "hours.worked"], y=pdata[pdata$Country==cn, "Total.Arrivals"], type="p", pch=19, add=T, col="blue")
  model <- lm(Total.Arrivals ~ hours.worked, data=pdata[pdata$Country==cn, ])
  abline(model, col="red")
  info <- summary(model)$coefficients
  p.val <- info[, "Pr(>|t|)"][2]
  legendary2("Regression line", col="red")
  mtext(cn, adj=0, line=2, cex=1.2, col="dodgerblue1")
  mtext(paste("p - value for regressor:", round(as.numeric(p.val), 3)), line=1, adj=0, col="salmon1")
  
}


```


```{r, echo=F, fig.cap="Relationship between changes in terror attacks and number of arrivals", fig.height=10, fig.width=8,  fig.pos="!ht", fig.align="center"}

ppdata <- pdata[abs(pdata$Terror.attacks)<100,  ]
 
par(mfrow=c(4, 3))

for(cn in unique(pdata$Country)){
  
  grid.frame(x=ppdata$Terror.attacks, y=ppdata$Total.Arrivals, xlab="Terror attacks", ylab="Total arrivals")
  matplot(x=ppdata[ppdata$Country==cn, "Terror.attacks"], y=ppdata[ppdata$Country==cn, "Total.Arrivals"], type="p", pch=19, add=T, col="blue")
  model <- lm(Total.Arrivals ~ Terror.attacks, data=ppdata[ppdata$Country==cn, ])
  abline(model, col="red")
  info <- summary(model)$coefficients
  p.val <- info[, "Pr(>|t|)"][2]
  legendary2("Regression line", col="red")
  mtext(cn, adj=0, line=2, cex=1.2, col="dodgerblue1")
  mtext(paste("p - value for regressor:", round(as.numeric(p.val), 3)), line=1, adj=0, col="salmon1")
  
}

```


```{r ,  echo=F, fig.cap="Infrastructure vs total arrivals", fig.height=10, fig.width=8,  fig.pos="!ht", fig.align="center"}
 
par(mfrow=c(4, 3))

for(cn in unique(pdata$Country)){
  
  grid.frame(x=pdata$Investment, y=pdata$Total.Arrivals, xlab="Investment to infrastructure", ylab="Total arrivals")
  matplot(x=pdata[pdata$Country==cn, "Investment"], y=pdata[pdata$Country==cn, "Total.Arrivals"], type="p", pch=19, add=T, col="blue")
  model <- lm(Total.Arrivals ~ Investment, data=pdata[pdata$Country==cn, ])
  abline(model, col="red")
  info <- summary(model)$coefficients
  p.val <- info[, "Pr(>|t|)"][2]
  legendary2("Regression line", col="red")
  mtext(cn, adj=0, line=2, cex=1.2, col="dodgerblue1")
  mtext(paste("p - value for regressor:", round(as.numeric(p.val), 3)), line=1, adj=0, col="salmon1")
  
}


```

```{r , echo=F,  fig.cap="Taxes vs total arrivals", fig.height=10, fig.width=8,  fig.pos="!ht", fig.align="center"}
 
par(mfrow=c(4, 3))

for(cn in unique(pdata$Country)){
  
  grid.frame(x=pdata$tax, y=pdata$Total.Arrivals, xlab="Taxes on goods on services", ylab="Total arrivals")
  matplot(x=pdata[pdata$Country==cn, "tax"], y=pdata[pdata$Country==cn, "Total.Arrivals"], type="p", pch=19, add=T, col="blue")
  model <- lm(Total.Arrivals ~ tax, data=pdata[pdata$Country==cn, ])
  abline(model, col="red")
  info <- summary(model)$coefficients
  p.val <- info[, "Pr(>|t|)"][2]
  legendary2("Regression line", col="red")
  mtext(cn, adj=0, line=2, cex=1.2, col="dodgerblue1")
  mtext(paste("p - value for regressor:", round(as.numeric(p.val), 3)), line=1, adj=0, col="salmon1")
  
}


```

\newpage

# References

```{r references, results="asis", echo=F, warning=F}
# PrintBibliography(bib, .opts = list(bib.style = "numeric"))
PrintBibliography(bib)
```



