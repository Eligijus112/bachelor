---
header-includes:
  - \usepackage[L7x]{fontenc}
bibliography: bibliography.bib
output:
 pdf_document: 
    fig_caption: yes
    keep_tex: yes
    number_sections: no
---

```{r setup, include=FALSE}
library(plm)
library(RCurl)
library(dplyr)
library(plyr)
# library(emibase)
library(plotrix)
# library(mgcv)
library(reshape2)
library(readxl)
library(knitr)
library(rworldmap)
library(cshapes)
library(countrycode)
library(texreg)
library(ggplot2)
library(directlabels)
# library(splines)
library(MASS)
library(TTR)
library(rsdmx)
library(httr)
library(splines)
library(xtable)
# library(dynpanel)
library(devtools)
library(corrplot)
source('functions.R')
library(car)
library(tseries)
```

\vskip 20pt
\centerline{\bf \large VILNIUS UNIVERSITY}
\bigskip
\centerline{\large \textbf{FACULTY OF MATHEMATICS AND INFORMATICS}}
\vskip 120pt
\centerline{\bf \Large \textbf{BACHELOR THESIS}}
\vskip 50pt
\begin{center}
{\bf \LARGE Tourism model in OECD countries}

\vspace{4mm}

\end{center}
\vskip 120pt
\centerline{\Large Eligijus Bujokas}
\vskip 120pt
\centerline{\large \textbf{VILNIUS (2016.12.07)}}

\newpage

\begin{titlepage}
\centerline {\bf \large FACULTY OF MATHEMATICS AND INFORMATICS}
\centerline {\bf DEPARTMENT OF MATHEMATICAL ANALYSIS}
\vskip 120pt
\large Scientific supervisor Vaidotas Zemlys - Balevičius, PhD \underline{\hskip 95pt}
\vskip 150pt
\end{titlepage}

\newpage

\begin{center}{\large\textbf{Turizmo modelis OECD šalyse}}\end{center}
\begin{small}
\vspace{2\baselineskip}
\begin{center}\textbf{Santrauka}\end{center}

Šio darbo tikslas yra sudaryti turizmo modelį OECD šalims. Naudojantis pasaulinio banko (World Bank), OECD ir Merilando Universiteto duomenų baze (the Department of Homeland Security Center of Excellence led by the Maryland University) duomenimis nustatėme, kad turistų skaičius tiesiškai priklauso nuo teroristinių įvykių skaičiaus, kaimyninių valstybių metinių darbo valandų skaičiaus ir nuo BVP (bendrasis vidaus produktas) vienam asmeniui. Tiesinės priklausomybės įrodymui naudojome panelinį modelį.

\vspace{\baselineskip}
\noindent\textbf{Raktiniai žodžiai :}
 BVP, OECD, pasaulinis bankas, panelinis modelis, darbo valandos, terorizmas, turizmas
\end{small}
\vspace{\baselineskip}
\thispagestyle{empty}
\begin{center}{\large\textbf{Tourism model in OECD countries}}\end{center}
\begin{small}
\vspace{2\baselineskip}
\begin{center}\textbf{Abstract}\end{center}
The aim of this paper is to construct a model to help explain the number of tourist arrivals in OECD countries. By using data from the World Bank, OECD and the Department of Homeland Security Center of Excellence led by the Maryland University's database we have found that the number of tourist arrivals to a certain OECD country is linearly dependant from GDP per capita, terrorist acts and the number of annual hours worked in neighbouring countries. To proove the linear dependance we used a panel model.

\vspace{\baselineskip}
\noindent\textbf{Key words :}
GDP per capita, OECD, World Bank, panel model, hours worked, terrorizm, tourism 
\end{small}
\vspace{\baselineskip}

\newpage

\tableofcontents


```{r constants, include=FALSE}
years.to.survey <- 1995:2015
path <- "data/"
# regions <- read.csv(file="input/regions.csv", stringsAsFactors = F)
createdir("plots")
# createdir("input")
createdir("data")
createdir("output")
# devtools::install_url("http://cran.r-project.org/src/contrib/rprojroot_1.2.tar.gz") ## MAY RESOLVE TROUBLESHOOTS
knitr::opts_chunk$set(fig.pos = 'h')
```

```{r data download and reading it, include=FALSE, echo=F}
# we will save this data to the 'data' folder
download.terror(years.to.survey, "data/") 
dt <- read.terror("data/terror/")
dt <- arrange(dt, Date)
na.index <- which(apply(dt, 1, function(x){ all(is.na(x)) }))
if(length(na.index)!=0) dt <- dt[-na.index, ]
write.csv(dt, file="output/terror raw data.csv", na="", row.names=F)

## Tourism data

dt.tour <- download.tourism("data/")

## Economic data

dt.eco <- download.economy("data/")
dt.eco <- make.economy.great.again(dt.eco)

## OECD data on hours worked

dt.oecd <- get_dataset("ANHRS", path="data/")
decode  <- get_decoder("data/")  

# OECD data on taxes

dt.tax <- read.csv("input/tax.csv")
dt.tax <- rename(dt.tax, c("ï..LOCATION" = "Country"))
dt.tax <- dt.tax[dt.tax$MEASURE %in% c("PC_GDP"), ]

dt.tax <- ddply(dt.tax, ~Country + TIME, function(xframe){
  
  xframe <<- xframe
    
  real.name <-  decode[decode$CODE==as.character(xframe$Country[1]), "Country"]
  if(length(real.name)==0) real.name <- xframe$Country[1]
  xframe$Country <- real.name
  return(xframe[nrow(xframe), ])
  
})

# dt.tax <- ddply(dt.tax, ~Country, function(xframe){
#   
#   xframe <<- xframe
#   
#   if(length(setdiff(years.to.survey, xframe$TIME))!=0){
#     
#     to.add <- matrix(nrow=length(setdiff(years.to.survey, xframe$TIME)), ncol=dim(xframe)[2]) %>% as.data.frame()
#     colnames(to.add) <- colnames(xframe) 
#     to.add$sum <- NA
#     to.add$TIME <- setdiff(years.to.survey, xframe$TIME)
#     to.add$Country <- xframe$Country[1]
#     xframe <- rbind(xframe, to.add)
#   }
#   
#   xframe <- arrange(xframe, TIME)
#    
#   return(xframe)
#   
# })

dt.tax <- dt.tax[, c("Country", "TIME", "Value")]
dt.tax <- rename(dt.tax, c("TIME"= "Date", "Value"="Tax"))
dt.tax[dt.tax$Country=="KOR", "Country"] <- "South Korea"
# Decoding country names

dt.oecd <- ddply(dt.oecd, ~COUNTRY + obsTime + EMPSTAT, function(xframe){
  
  xframe <<- xframe
  real.name <-  decode[decode$CODE==xframe$COUNTRY[1], "Country"]
  if(length(real.name)==0) real.name <- xframe$COUNTRY[1]
  xframe$COUNTRY <- real.name
  return(xframe)
  
})

dt.oecd <- dt.oecd[dt.oecd$EMPSTAT=="TE", ]
dt.oecd[dt.oecd$COUNTRY=="KOR", "COUNTRY"] <- "South Korea"
# dt.oecd[dt.oecd$COUNTRY=="Slovak Republic", "COUNTRY"] <- "Slovakia"
```

```{r data for modelling, include=FALSE, echo=F}

# In this section we will be creating a data frame for a VAR model or a panel model

master.data <- aggregate.terror.by.country(dt)

dt.tour <- rename(dt.tour, c("Year" = "Date", "CountryName" = "Country"))
dt.eco <- rename(dt.eco, c("Year" = "Date", "CountryName" = "Country"))

dt.tour$Country <- gsub("Korea, Rep.", "South Korea", dt.tour$Country)
dt.eco$Country <- gsub("Korea, Rep.", "South Korea", dt.eco$Country)

master.data <- merge(master.data, dt.tour)

for(prod in unique(dt.eco$Indicator)){

  tmp <- dt.eco[dt.eco$Indicator==prod, c("Country", "Date", "value")]
  tmp <- rename(tmp, c("value" = prod))
  master.data <- merge(master.data, tmp)

}

# Merging with OECD data

dt.oecd <- plyr::rename(dt.oecd, c("COUNTRY" = "Country", "obsTime" = "Date", "obsValue" = "Working hours"))
dt.oecd <- dt.oecd[, c("Country", "Date", "Working hours")]

master.data <- merge(master.data, dt.oecd)
master.data <- merge(master.data, dt.tax)

# master.data <- ddply(master.data, ~Country, function(xframe){
#   
#   xframe <<- xframe
#   
#   xframe[, 4:dim(xframe)[2]] <- apply( xframe[, 4:dim(xframe)[2]], 2, MASplineVector )
#   
#   return(xframe)
#   
# })
# Dropping incomplete rows

master.data <- master.data[complete.cases(master.data), ]

master.data <- arrange(master.data, Country)

# Adding number of neighbours

# neighbours <- find.neighbours(unique(master.data$Country), path=path)
# 
# master.data <- merge(master.data, neighbours, sort=F)
# master.data$Numb.neighbour <- as.numeric(as.character(master.data$Numb.neighbour))


master.data <- plyr::rename(master.data, c('GDP per capita (current US$)' = "GDP.per.capita"))
master.data <- plyr::rename(master.data, c('Working hours' = "Hours.worked"))

## Dropping some countries

master.data <- master.data[master.data$Country != "Costa Rica", ]
master.data <- master.data[master.data$Country %in% master.data[master.data$Date=="1995", "Country"], ]
master.data <- master.data[master.data$Date %in% 1995:2014, ]

structure.data <- read.csv("data/tourism/tourism structure.csv")
structure.data <- structure.data %>% dplyr::select(Country, Variable, Year, Value)
structure.data$Country <- gsub("Korea", "South Korea", structure.data$Country)
structure.data$Variable <- gsub("Korea", "South Korea", structure.data$Variable)
structure.data <- structure.data[structure.data$Variable %in% dt.oecd$Country & structure.data$Country %in% dt.oecd$Country, ]
structure.data <- rename(structure.data, c("Value" = "Tourist.arrivals", "Year"="Date"))

## Local economic variables

structure.data <- merge(structure.data, master.data[, c("Country", "Date", "GDP.per.capita")])
structure.data <- merge(structure.data, master.data[, c("Country", "Date", "Terror.attacks")])
structure.data <- merge(structure.data, master.data[, c("Country", "Date", "Tax")])

## Adding the appropriate economic variables

structure.data <- ddply(structure.data, ~Country + Date, function(xframe){
  
  xframe <<- xframe
  xframe$GDP.abroad <- sum(dt.eco[dt.eco$Country %in% xframe$Variable & dt.eco$Date==xframe$Date[1], "value"])
  xframe$hours.abroad <- mean(dt.oecd[dt.oecd$Country %in% xframe$Variable & dt.oecd$Date==xframe$Date[1], "Working hours"])
  
  return(xframe[nrow(xframe), ])
})

structure.data$Variable <- NULL
p.structure <-  plm.data(structure.data, index = c("Country", "Date")) 
log.vars <- c("GDP.per.capita", "GDP.abroad", "hours.abroad", "Tourist.arrivals")
p.structure[, log.vars] <- apply(p.structure[, log.vars], c(1, 2), log)
p.structure[, 3:dim(p.structure)[2]] <- apply(p.structure[, 3:dim(p.structure)[2]], 2, function(x){
  
  return(c(NA, diff(x)))
  
  })

model.naive <- plm(Tourist.arrivals ~  Tax +  GDP.abroad + hours.abroad, data=p.structure, model = "within")
summary(model.naive)
```

\newpage

\section{Introduction}

Wikipedia gives the following definition regarding what is tourism [see @tourism.definition]: "Tourism is travel for pleasure; also the theory and practice of touring, the business of attracting, accommodating, and entertaining tourists, and the business of operating tours". According to UNWTO (United Nations World Tourism Organization)[see @UNWTO]:

$\bullet$ 1 in 11 jobs worldwide are related to the tourism industry.

$\bullet$ 10 percent of total world GDP is produced by tourism.

$\bullet$ Tourism made up for about 7 percent of world's exports: 1.5 trillion US dollars.

$\bullet$ In 2015 there were 1186 million tourists worldwide and this number is expected to grow up to 1.8 billion by 2030.

$\bullet$ In 2015 the total receipts by tourists was about 1.26 billion US dollars.

From these figures it is clear to see that every country can benefit from an increase in tourism flows. Tourism continues to demonstrate its key role in generating economic activity, employment and export revenues in the OECD area, where it directly contributes, on average, 4.1 \% of GDP, 5.9 \% of employment and 21.3 \% of service exports. Tourism offers strong potential to support job rich growth and at around 80 percent, tourism exports also generate higher than average domestic value added. International tourist arrivals surpassed 1.1 billion in 2014 (World Tourism Organization), following a resurgence in arrivals to OECD countries (6.4 \%), which increased at a faster rate than the global average (4.2 \%) (as of 2016). 

In this paper we want to find which economic or social variables significantely influence the tourism flows to help governments in deciding which policies to adapt in order to grow the tourism sector.



\newpage

\section{Countries}

The countries which will be analyzed are all members of the OECD (\textbf{Organisation for Economic Co-operation and Development}) [see @OECD]. Currently there are 35 member states but in this analysis we will be adding Lithuania because it is strongly believed that this country will soon join OECD. 

```{r map of OECD countries, echo=F, fig.width=8, fig.height=8, fig.cap= "Map of the OECD countries", echo=F, warning=F, include=T}

OECD.cn <- data.frame(country = unique(master.data$Country), 
                      OECD = rep.int("OECD countries", 
                                     length(unique(master.data$Country))))
malMap <- joinCountryData2Map(OECD.cn, joinCode = "NAME",
  nameJoinColumn = "country", verbose = F, suggestForFailedCodes = F)

mapCountryData(malMap, nameColumnToPlot="OECD", catMethod = "categorical",
  missingCountryCol = gray(.8), oceanCol = 'cyan', mapTitle = "Political map of the world")


```

As you can see from the map, not all 36 countries mentioned are visualized. This is because they are dropped due to lack of data. 

\newpage

\subsection{Total number of arrivals}

To have a better picture of the tourist flows to the OECD countries we can visualize the data.

```{r arrivals visualization, echo=F, fig.cap="Total number of tourist arrivals", fig.height=10, fig.width=11, fig.pos="H"}

dt.a <- master.data[, c("Country", "Date", "Total.Arrivals"), ]

dt.a$Date <- as.numeric(dt.a$Date)
dt.a$Total.Arrivals <- dt.a$Total.Arrivals/1000

p <- ggplot(dt.a, aes(x=Date, y=Total.Arrivals, group=Country, colour=Country)) + geom_line(size=1.2) + ylab("Total Arrivals, thousands") + theme_bw()  + theme(legend.position="none") + scale_x_continuous(expand = c(0, 3)) + geom_dl(aes(label = Country), method = list(dl.combine("last.bumpup"), cex = 1)) 

plot(p)

```

To see the overal trend from all of these countries is quite difficult so we will plot the sum of tourist arrivals in each year.

```{r arrival trend, echo=F, fig.cap="Trend of tourists arrivals", fig.height=6, fig.width=7,  fig.pos="H"}

dt.a <- master.data[, c("Country", "Date", "Total.Arrivals"), ]

dt.a <- ddply(dt.a, ~Date, function(xframe){
  
  xframe <<- xframe
  
  results <- data.frame(Country='OECD', Date=xframe$Date[1], Total.Arrivals=sum(xframe$Total.Arrivals, na.rm=T))
  xframe <- rbind(xframe, results)
  return(xframe[nrow(xframe), ])

})

dt.a$Total.Arrivals <- dt.a$Total.Arrivals/1000
dt.a$Date <- as.numeric(dt.a$Date)

p <- ggplot(dt.a, aes(x=Date, y=Total.Arrivals, group=Country, colour=Country, label=Country)) + geom_line(size=1.2) + ylab("Total Arrivals, thousands") + theme_bw() + scale_x_continuous(expand = c(0, 4)) + geom_dl(aes(label = Country), method = list(dl.combine("first.polygons"), cex = 1)) + geom_dl(aes(label = Country), method = list(dl.combine("last.polygons"), cex = 1)) + stat_smooth(se = FALSE, method = "lm", formula = y ~ ns(x,1),aes( colour="Linear trend"), linetype="dashed")  + scale_colour_manual(values = c("coral3", "darkblue"),
                       guide = guide_legend(override.aes = list(
                         linetype = c("solid", "dashed"),
                         shape = c(16, 16)), title="Legend"))

plot(p)

```

From figure 3 we can clearly see that the overal sum of tourist arrivals through the years is increasing rapidly. In this paper we will try to model these arrival values and identify the biggest factors of this process. 

\newpage

\subsection{Distribution of arrivals}

It is interesting to see how the distribution of tourism flows changed (or not) through the years. We will divide OECD countries to several regions based on GDP per capita of the most recent year and analyze how the numbers differed between the regions. 

```{r getting clusters, echo=F, fig.cap="Clusters of countries", fig.height=6, fig.width=7,  fig.pos="H"}

data.for.clust <- master.data[master.data$Date=="2014", c("Country", "GDP.per.capita")]
set.seed(20)

fit <- kmeans(data.for.clust$GDP.per.capita, 6)
data.for.clust$cluster <- fit$cluster

OECD.cn <- data.frame(country = unique(data.for.clust$Country), 
                      cluster = data.for.clust$cluster)
malMap <- joinCountryData2Map(OECD.cn, joinCode = "NAME",
  nameJoinColumn = "country", verbose = F, suggestForFailedCodes = F)

mapCountryData(malMap, nameColumnToPlot="cluster", catMethod = "categorical",
  missingCountryCol = gray(.8), oceanCol = 'cyan', mapTitle = "Political map of the world")

```

```{r getting distributions, echo=F, message=F, warning=F, fig.cap="Distribution of arrivals", fig.height=6, fig.width=7,  fig.pos="H"}

ggdata <- data.frame()

for(yy in c('1998', '2003', '2008', '2013')){
  data.for.distr <- master.data[master.data$Date==yy, c("Country", "GDP.per.capita", "Total.Arrivals")]
  data.for.distr <- left_join(data.for.distr, data.for.clust[, c("Country", "cluster")])
  
  ## Calculating the total shares of arrivals, percent
  
  data.for.distr$Total.Arrivals <- data.for.distr$Total.Arrivals*100/sum(data.for.distr$Total.Arrivals)
  
  data.for.distr <- ddply(data.for.distr, ~cluster, function(xframe){
    
    xframe <<- xframe
    
    add.frame <- xframe[1, ]
    add.frame$Country <- xframe$cluster[2]  
    add.frame$Total.Arrivals <- sum(xframe$Total.Arrivals, na.rm=T) 
    
    xframe <- rbind(xframe, add.frame)
    
    xframe <- xframe[nrow(xframe), c("Country", "Total.Arrivals")]
  
    return(xframe)  
  })
  
  ##Sorting the clusters based on the center values
  
  cluster.arranged <- fit$centers %>% as.data.frame()
  cluster.arranged$name <- seq(1, by=1, length.out = dim(cluster.arranged)[1])
  cluster.arranged <- plyr::rename(cluster.arranged, c("V1" = "value", "name" = "cluster"))
  cluster.arranged <- arrange(cluster.arranged, value)
  
  data.for.distr <- merge(data.for.distr, cluster.arranged)
  data.for.distr <- arrange(data.for.distr, value)
  data.for.distr$Year <- yy
  
  ggdata <- rbind(ggdata, data.for.distr)

}

  # p <- ggplot(ggdata, aes(x=value, y=Total.Arrivals, group=Year, colour=Year)) + geom_line(size=1.2, col="cornflowerblue") + ylab("Share of total arrivals, percent") + xlab("Mean of cluster") + theme_bw() + ggtitle(paste("Year", yy)) + geom_point(size=2, col="dodgerblue3")
  # plot(p)

p <- ggplot(ggdata, aes(x=value, y=Total.Arrivals, group=Year, colour=Year)) + geom_line(size=1.2) + theme_bw() 
p <- p + ylab("Share of total arrivals, percent") + xlab("Mean of cluster")
plot(p)
```

As we can see from figure, at least visually the distribution of the number of arrivals between countries "evens up" as time goes by.

\newpage
\section{Economic variable selection}

The aim of this paper is to create a model to help explain and forecast the total number of arrivals of tourists to a given OECD country. We want to find the function \textbf{f} in the equation:

\[
log(A)_{it}  = \textbf{f}(\Delta log(GDP)_{it}, \Delta terror_{it}, \Delta log(hours)_{it})
\]

Here 

$A_{it}$ - total number of arrivals to a country i at time period t, thousands

$GDP_{it}$ - GDP per capita at a country i at time period t, thousands 

$terror_{it}$ - total number of terrorist attacks at country i at time period t

$hours_{it}$ - average numbers of hours worked annualy of country's i neighbors at time period t

<!-- \subsection{Motivation behind variable selection} -->

\newpage

\subsection{Gross domestic product per capita}

The variable $GDP$ was chosen to reflect the economic situation and the standart of living in a given country. GDP per capita is calculated by taking the  total output of a country and dividing it by the number of people in the country. The division by the number of people helps us to compare economies between countries. 

```{r OECD GDP per capita, echo=F, fig.cap="GDP per capita of OECD", fig.height=6, fig.width=7,  fig.pos="H"}

dt.a <- master.data[, c("Country", "Date", "GDP.per.capita")]

dt.a <- ddply(dt.a, ~Date, function(xframe){
  
  xframe <<- xframe
  
  results <- data.frame(Country='OECD', Date=xframe$Date[1], GDP.per.capita=mean(xframe$GDP.per.capita, na.rm=T))
  xframe <- rbind(xframe, results)
  return(xframe[nrow(xframe), ])

})

dt.a <- rename(dt.a, c("GDP.per.capita" = "value"))
dt.a$Date <- as.numeric(dt.a$Date)

p <- ggplot(dt.a, aes(x=Date, y=value, group=Country, colour=Country, label=Country)) + geom_line(size=1.2) + ylab("GDP per capita, USD") + theme_bw() + scale_x_continuous(expand = c(0, 4)) + geom_dl(aes(label = Country), method = list(dl.combine("first.polygons"), cex = 1)) + geom_dl(aes(label = Country), method = list(dl.combine("last.polygons"), cex = 1))  + stat_smooth(se = FALSE, method = "lm", formula = y ~ x,aes( colour="Linear trend"), linetype="dashed") +
scale_colour_manual(values = c("darkblue", "coral3"),
                       guide = guide_legend(override.aes = list(
                         linetype = c("dashed", "solid"),
                         shape = c(16, 16)), title="Legend")) 

plot(p)

```

\newpage

\subsection{Number of terrorist attacks}

In recent years there has been an increase in terrorist attacks in the OECD member countries: France, Germany, Turkey and others have suffered attacks, especially from the group called ISIS. It is quite straightforward to think that in light of these attacks tourists will be trying to avoid countries with a higher chance of a terrorist attack. We will want to know whether terrorist attacks is a significant variable to tourist arrivals and how much of an impact it has. 

```{r visualizing terror, echo=F, fig.cap="Terrorism in OECD countries", fig.height=6, fig.width=7,  fig.pos="H"}

dt.a <- master.data[, c("Country", "Date", "Terror.attacks")]

dt.a <- ddply(dt.a, ~Country, function(xframe){
  
  xframe <<- xframe
  
  results <- data.frame(Country=xframe$Country, Date="Total", Terror.attacks=sum(xframe$Terror.attacks, na.rm=T))
  xframe <- rbind(xframe, results)
  return(xframe[nrow(xframe), ])

})

dt.a <- t(dt.a[, c("Country", "Terror.attacks")])
# dt.a[2, ] <- as.numeric(dt.a[2, ])
nn <- dt.a[1, ]
dt.a <- dt.a[-1, ] %>% as.data.frame() %>% t()
dt.a <- apply(dt.a, c(1,2), function(x) {as.numeric(x)})
names(dt.a) <- nn
dt.a <- sort(dt.a)
par(mai=c(1,2,1,1))
barplot(dt.a , col='deepskyblue3',  horiz=T , las=1, cex.names = 0.7, main="Total number of attacks, 1995 - 2014")

```

\newpage

\subsection{Average annual working hours}

Most countries in the developed world have seen average hours worked decrease significantly. This is due because of the effectiveness of new tools and technologies, overall population growth and other factors.

```{r visualizing working hours, echo=F, fig.cap="Hours worked in OECD countries", fig.height=6, fig.width=7,  fig.pos="H"}

dt.a <- master.data[, c("Country", "Date", "Hours.worked")]

dt.a <- ddply(dt.a, ~Date, function(xframe){
  
  xframe <<- xframe
  
  results <- data.frame(Country='OECD', Date=xframe$Date[1], Hours.worked=mean(xframe$Hours.worked, na.rm=T))
  xframe <- rbind(xframe, results)
  return(xframe[nrow(xframe), ])

})

dt.a <- rename(dt.a, c("Hours.worked" = "value"))
dt.a$Date <- as.numeric(dt.a$Date)

p <- ggplot(dt.a, aes(x=Date, y=value, group=Country, colour=Country, label=Country)) + geom_line(size=1.2) + ylab("Average hours worked annualy") + theme_bw() + scale_x_continuous(expand = c(0, 4)) + geom_dl(aes(label = Country), method = list(dl.combine("first.polygons"), cex = 1)) + geom_dl(aes(label = Country), method = list(dl.combine("last.polygons"), cex = 1)) + stat_smooth(se = FALSE, method = "lm", formula = y ~ x,aes( colour="Linear trend"), linetype="dashed") +
scale_colour_manual(values = c("darkblue", "coral3"),
                       guide = guide_legend(override.aes = list(
                         linetype = c("dashed", "solid"),
                         shape = c(16, 16)), title="Legend")) 

plot(p)

```

As we can see from figure 6 the overal downwards trend of the working annaul working hours' average is true to the OECD countries.

```{r creating a new hours worked variable, include=FALSE, echo=F}

## For this analysis we will be creating a new variable regarding the average of hours worked in the neighboring countries of a given country
## This variable helps to explain the impact of the decline in the average working hours for the total number of arrivals to a given country

dmat <- read.csv(paste0(path, "GeoCountries/distances.csv"), header = F)
colnames(dmat) <- dmat[1, ]
rownames(dmat) <- dmat[1, ]
dmat <- dmat[-1, -1] 

dmat <- as.data.frame(dmat)
  
names.on.file <- colnames(dmat)
decode <- merge(data.frame(cown=as.numeric(names.on.file)), countrycode_data[, c("country.name.en", "cown")], sort=F)
  
decode$country.name <- gsub("Republic of Korea", "South Korea", decode$country.name)
decode$country.name <- gsub("Slovakia", "Slovak Republic", decode$country.name)
decode$country.name <- gsub("United Kingdom of Great Britain and Northern Ireland", "United Kingdom", decode$country.name)  
decode$country.name <- gsub("United States of America", "United States", decode$country.name)  

colnames(dmat) <- decode$country.name
rownames(dmat) <- decode$country.name


# leaving only OECD countries

dmat <- dmat[, which(colnames(dmat) %in% unique(master.data$Country))]
dmat <- dmat[which(rownames(dmat) %in% unique(master.data$Country)), ]

master.data <- ddply(master.data, ~Country, function(xframe){
  
  xframe <<- xframe
  
  # friend <- rownames(dmat)[which(dmat[, xframe$Country[1]]<500)] 
  # 
  # friend <- friend[-which(friend==xframe$Country[1])]
  #  
  # if(length(friend)==0){
    
    friend <- rownames(dmat)[which(dmat[, xframe$Country[1]] %in% sort(dmat[, xframe$Country[1]]))] 

    friend <- friend[-which(friend==xframe$Country[1])]
     
  # }

  new.ave <- master.data[master.data$Country %in% friend, ]
  
  new.ave <- ddply(new.ave, ~Date, function(yframe){
    
    yframe <<- yframe
    
    yframe[nrow(yframe), "Hours.worked"] <- mean(yframe$Hours.worked)
    
    yframe <- yframe[nrow(yframe), ]
    yframe$Country <- "OECD"
  
    return(yframe)  
  })
  
  dates <- intersect(new.ave$Date, xframe$Date)
  xframe[xframe$Date %in% dates, "Hours.worked"] <- new.ave[new.ave$Date %in% dates, "Hours.worked"]
  return(xframe)
  
})


```

\section{Modelling}

\subsection{Panel data structure}

Panel data provides information on individual behaviour, both across individuals and over time - the data has cross-sectional and time-series dimensions. Panel data includes N individuals over T regular time periods. We can denote a panel data model simmilary like a typical bivariate reggression model [see @econometric.methods, pp. 390]:

\[
\tag{1}
  y_{it} = \alpha_{i} + \beta X_{it} + u_{it}
\]

The difference from a typical reggression model is that there are the $i$ and the $t$ terms, where $i$ denotes the individual and $t$ denotes the time period. The construction of a panel model starts with appropriately 'stacking' the data set. Assuming that y is the dependant variable and X is the matrix of the explanatory variables we need to have the following structure [see @econometric.methods, pp. 388]: 

\[
  Y = y_{i} = \begin{bmatrix}
         y_{1}    \\
         y_{2}    \\[0.3em]
         \vdots   \\[0.3em]
         y_{N}     
  \end{bmatrix} = \begin{bmatrix}
         e    \\
         0    \\[0.3em]
         \vdots   \\[0.3em]
         0     
  \end{bmatrix}  \alpha_{1} + \begin{bmatrix}
       0    \\
       e    \\[0.3em]
       \vdots   \\[0.3em]
       0    
\end{bmatrix}  \alpha_{2} + \cdots + \begin{bmatrix}
       0    \\
       0    \\[0.3em]
       \vdots   \\[0.3em]
       e     \\[0.3em]
\end{bmatrix}  \alpha_{N} + \begin{bmatrix}
       x_{1}    \\[0.3em]
       x_{2}    \\[0.3em]
       \vdots   \\[0.3em]
       x_{n}     \\[0.3em]
\end{bmatrix} \beta +   \begin{bmatrix}
       u_{1}    \\[0.3em]
       u_{2}    \\[0.3em]
       \vdots   \\[0.3em]
       u_{n}     \\[0.3em]
\end{bmatrix}
\]

Here

\[
y_{i} = \begin{bmatrix}
       y_{i1}    \\
       y_{i2}    \\[0.3em]
       \vdots   \\[0.3em]
       y_{iT}     \\[0.3em]
\end{bmatrix} x_{i} = \begin{bmatrix}
       x_{1i1} & x_{2i1} & \cdots & x_{Ki1}    \\
       x_{1i2} & x_{2i2} & \cdots & x_{Ki2}   \\[0.3em]
       \vdots & \vdots & \cdots & \vdots   \\[0.3em]
      x_{1iT} & x_{2iT} & \cdots & x_{KiT}      \\[0.3em]
\end{bmatrix}
\]

For example, the 'head' and the 'tail' of the raw data which will be analysed looks like this:

```{r visualising data table, echo=F}

pdata <- plm.data(master.data, index = c("Country", "Date")) 

pdata$Total.Arrivals <- log(pdata$Total.Arrivals)
pdata$GDP.per.capita <- log(pdata$GDP.per.capita)
pdata$Hours.worked   <- log(pdata$Hours.worked)

pdata <- ddply(pdata, ~Country, function(xframe){
  
  xframe <<- xframe
  
  xframe[, 3:dim(xframe)[2]] <- apply(xframe[, 3:dim(xframe)[2]], 2, function(x){
    
    x <<- x
    
    x <- c(NA, diff(x))
    
    return(x)
    
  })
  
  return(xframe)
  
})

pdata <- pdata[pdata$Date!="1995", ]
kable(head(pdata), format = "latex", row.names = F)
kable(tail(pdata), format = "latex", row.names = F)

```

\newpage

<!-- \subsection{Correlation} -->

<!-- Before building a model for our data we need to check the correlation between variables. We will be calculating the Pearson's correlation coefficient [see @econometric.methods, pp. 8]. The Pearson's correlation between two random variables is defined as: -->

<!-- $$\rho = \dfrac{\mathbf{E}\left( (X - \mathbf{E}[X])(Y - \mathbf{E}[Y]) \right)}{\sigma_{X} \sigma_{Y}}$$ -->

<!-- Here: -->

<!-- $\mathbf{E [\centerdot]}$ - expected value of $\centerdot$. -->

<!-- $\sigma_{\centerdot}$ - standart deviation of $\centerdot$. -->

<!-- The $\rho$ coefficient for the sample is equal to: -->

<!-- $$\rho = \dfrac{\sum_{i=1}^{n}\left( (x_{i} - \overline{x})(y_{i} - \overline{y}) \right) }{\sqrt{\sum_{i=1}^{n}\left( (x_{i} - \overline{x})\right)}  \sqrt{\sum_{i=1}^{n}\left( (y_{i} - \overline{y})\right)} }$$ -->

<!-- ```{r visualizing correlation matrix, echo=F, fig.cap="Correlation visualization", fig.height=8, fig.width=7,  fig.pos="H"} -->

<!-- # cn <- c("France", "United Kingdom") -->
<!-- #  -->
<!-- # par(mfrow=c(2,1)) -->

<!-- # for(cc in cn){ -->
<!--   # for.corr <- master.data[, c("Terror.attacks", "Total.Arrivals", "GDP.per.capita", "Hours.worked")] -->
<!--   # M <- cor(for.corr) -->
<!--   # corrplot(M, method="color", order="hclust", addCoef.col = "black", type="lower", title = paste("Correlation diagram for", cc), mar=c(0,3,1,0)) -->
<!-- # } -->

<!-- ``` -->

<!-- As we can see from figure 7, terror attacks for both countries have a negative correlation with total arrivals, GDP per capita has a strong positive correlation and the hours worked variable has a strong negative effect. This trend is simmilar to all countries and thus one of the "good" model criteria will be to have the following coefficients: -->

<!-- $\bullet$ Number of terrorist attacks should have a negative impact to tourist arrivals. -->

<!-- $\bullet$ GDP per capita should have a positive impact to tourist arrivals. -->

<!-- $\bullet$ Hours worked variable should have a negative impact to tourist arrivals. -->


\newpage

\subsection{Unit roots}

Before modelling we need to check whether there are unit roots in our data set. We will use the \textit{purtest()} function to check for this. This function implements the Maddala and Wu test.

```{r adf, echo=T, warning=F, comment=NA}

Arrivals <- as.data.frame(split(pdata$Total.Arrivals, pdata$Country))
purtest(Arrivals, pmax = 4, exo = "intercept", test = "madwu")

GDP <- as.data.frame(split(pdata$GDP.per.capita, pdata$Country))
purtest(GDP, pmax = 4, exo = "intercept", test = "madwu")

Hours <- as.data.frame(split(pdata$Hours.worked, pdata$Country))
purtest(Hours, pmax = 4, exo = "intercept", test = "madwu")

Attacks <- as.data.frame(split(pdata$Terror.attacks, pdata$Country))
purtest(Attacks, pmax = 4, exo = "intercept", test = "hadri")

Infrastructure <- as.data.frame(split(pdata$Tax, pdata$Country))
purtest(Infrastructure, pmax = 4, exo = "intercept", test = "madwu")

```

The alternative hypothesis for the test is that the data does not have a unit root. According to the tests no variable has a unit root. The test does not show a statistic for the terrorist attacks variable because many variables there are equal to 0. We wil conclude that the variable is stationary for all countries. 

\newpage

\subsection{Multicolinearity}

Multicollinearity refers to predictors that are correlated with other predictors. In other words, the correlation between the explanatory variables are high. Severe multicollinearity is a major problem, because it increases the variance of the regression coefficients, making them unstable. To test whether there is a multicollinearity problem between the variables that we have chosen, we will regress the variables one with another and check whether one significantly influences another.

```{r multicolinearity, echo=F, warning=F, comment=NA}

model1 <- lm(GDP.per.capita ~ Hours.worked, data=pdata)
model2 <- lm(Hours.worked ~ GDP.per.capita, data=pdata)
model2 <- lm(Hours.worked ~ Infrastructure, data=pdata)

```

```{r multicolinearity results, results="asis", echo=F, warning=F}
tab <- xtable(summary(model1)$coef, digits=c(3, 3, 3, 3, 3), caption = "OLS model")
print(tab, type="latex", floating = T, only.contents = F, comment = F)

tab <- xtable(summary(model2)$coef, digits=c(3, 3, 3, 3, 3), caption = "OLS model")
print(tab, type="latex", floating = T, only.contents = F, comment = F)
```
The variable regarding terrorist attacks has no correlation with the others, but hours worked and GDP per capita show a strong correlation with one another. According to the models, a one percent change in the hours worked variable would lead to the change of GDP per capita for 6.628 percent. To deal with the multicolinearity problem we wil drop the hours worked variable and continue without it.

\newpage

\subsection{Pooling estimator}

The first model which we will be examining will be the simple OLS model which ignores the panel data structure. The equation is then:

\[
y = \alpha + X \beta + \epsilon
\]

And the estimator is derived solving the equation:

\[
\widehat{\beta} = \left( X X^{T} \right)^{-1}X^{T}y
\]

```{r pooling panel model, include=FALSE, echo=F, warning=F}

model.ols <- lm(formula = Total.Arrivals ~ Terror.attacks + Hours.worked + Tax, data=pdata)
# summary(model.ols)
```

```{r xtable, results="asis", echo=F, warning=F}
tab <- xtable(summary(model.ols)$coef, digits=c(3, 3, 3, 3, 3), caption = "OLS model")
print(tab, type="latex", floating = T, only.contents = F, comment = F)
```

The problem with this model is that while giving consistent estimators, the standart errors of those estimators will be understated. Also, OLS is not efficient compared to the generalized least-squares procedure [see @econometric.methods, pp. 391].

\newpage

\subsection{Fixed effects model}

In this section we will be creating a fixed effects panel data model [see @econometric.methods, pp. 395 - 399]. Recall equation (1). The $\alpha_{i}$ is called the \textit{individual effect}.
If $\alpha_{i}$ does correlate with $X_{it}$ then the panel model can be reffered to as a fixed effects model. 

\subsubsection{Model assumptions}

Let as assume that we have n individuals and T observations for each individual. The random effects model has the following structure:

$$y_{it} = X_{it} \beta + \epsilon_{it} $$
$$\epsilon_{it} = \alpha_{i} + \eta_{it} $$

The assumptions for the errors are:

$$\mathbf{E}[\eta] = 0 ; E[\eta\eta^{\prime}] = \sigma^{2}_{\eta}I_{nT}$$

$$\mathbf{E}[\alpha_{i}\alpha_{j}] = 0, \forall i\neq j;  E[\alpha\alpha^{\prime}] = \sigma^{2}_{\alpha}$$

$$\mathbf{E}[\alpha_{i}\eta_{j}] = 0, ;  E[\alpha] = 0$$

All of the above expectations are conditional on $X$. In the fixed effects model case the $\alpha_{i}$ term is treated as an unknown parameter and has to be estimated. 

\subsubsection{Obtaining estimates}

To obtain the $\widehat{\beta_{FE}}$ we first "demean" the data. $\forall i$:

$$y_{it} - \overline{y_{i}} = \left(X_{it} -  \overline{X_{i}}\right) \beta + \left( \alpha_{i}  - \overline{\alpha_{i}}\right) + \left( \epsilon_{i}  - \overline{\epsilon_{i}}\right)$$

Now since $\overline{\alpha_{i}}$ is just $\alpha_{i}$ then the above equations simplifies to:

$$\dot{y} := y_{it} - \overline{y_{i}} = \left(X_{it} -  \overline{X_{i}}\right) \beta + \left( \epsilon_{i}  - \overline{\epsilon_{i}}\right) =: \dot{X} \beta_{FE} + \dot{\epsilon}$$

Now to obtain the estimate $\widehat{\beta_{FE}}$ we simply run an OLS procedure to the equation above. The estimator $\widehat{\beta_{FE}}$ is often called the within estimator. Any procedure that eliminates the term $\alpha_{i}$ can be called a $\textit{within transformation}$. To obtain the $alpha_{i}$ for every $i$ we simply match the intercept coordinates that is estimated by running the reggresion for the demeaned data with the way the data was stacked. 


\subsubsection{Results}

```{r fixed effects model, include=FALSE, echo=F}

model.fixed <- plm(formula = Total.Arrivals ~ Terror.attacks + Hours.worked + Tax, data=pdata, model="within")
# summary(model.fixed)
```

```{r, results='asis', echo=F, warning=F}
  # texreg(model.fixed, caption = "Fixed effects model", custom.coef.names = c("attacks", "log GDP", "log Hours"), digits = 3, 
  #         custom.model.names = "Fixed effects model")

tab <- xtable(summary(model.fixed)$coef, digits=c(3, 3, 3, 3, 3), caption = "Fixed effects model")
print(tab, type="latex", floating = T, only.contents = F, comment = F)

```

<!-- As we can see from table 2 now the coefficients are logical - one terrorist attack can lead to about 0.5 percent decline in tourist arrivals. A one percent increase in GDP per capita can lead up to 0.5 increase in tourist arrivals and decreasing 1 percent of hours worked within neighboring countries leads to 0.43 percent more tourist arrivals. Table 2 does not include the intercept term and the the factor variable regarding its "goodness" of geographical place.  -->

As we can see from table 2 the coefficients have a simmilar signs to them as in the OLS case. A one percent increase to the GDP per capita variable would lead to about 7.5 percent increase to the total number of arrivals. On another hand, a single terrorist attack would decline the number of arrivals by about 0.2 percent.

The intercept is missing because every country $\textbf{i}$ has it's own intercept value and those values are not displayed in model output. This is the main goal of the fixed effects model because we get a different level for every country thus we take into account the heterogeneity across countries and time.

Applying the F test for individual effects gives the result:

```{r pFtest, echo=T, comment=NA}
  pFtest(model.fixed, model.ols)
```

According to these results the fixed effects model is not significantly better in explaining the number of arrivals compared to the OLS model.

```{r estimating ols and fixed model fits, echo=F, comment=NA}
  
  pdata <- ddply(pdata, ~Country + Date, function(xframe){
    
    xframe <<- xframe
    
    
    ## fixed effects
    
    intercept <- fixef(model.fixed)[xframe$Country[1]] %>% as.numeric()
    X <- xframe[, c('Terror.attacks', "GDP.per.capita")]
    fit <- model.fixed$coefficients[1] * X[1] + model.fixed$coefficients[2] * X[2] + intercept
    fit <- as.numeric(fit) # this is the fit of the logged number of arrivals
    xframe$fc.fixed.effect <- fit
    
    ## OLS
    
    X <- xframe[, c('Terror.attacks', "GDP.per.capita")] 
    fit <- model.ols$coefficients[2] * X[1] + model.ols$coefficients[3] * X[2] 
    fit <- fit + model.ols$coefficients[1] 
    xframe$fc.OLS <- as.numeric(fit)
    
    return(xframe)
  })

```


```{r visualizing the differences between fits, echo=F, fig.cap="OLS and fixed effects model comparison", fig.height=7, fig.width=8,  fig.pos="H"}
  
fdt <- pdata[, c("Country", "Date", "Total.Arrivals", "fc.fixed.effect", 'fc.OLS')]
# fdt$Total.Arrivals  <- exp(fdt$Total.Arrivals)/1000
# fdt$fc.fixed.effect <- exp(fdt$fc.fixed.effect)/1000
# fdt$fc.OLS          <- exp(fdt$fc.OLS)/1000
fdt$Date <- fdt$Date %>% as.character() %>% as.numeric()

## we will plot 4 countries in order to have a clear picture
cn <- c( "Australia",  "Austria", "France", "Lithuania")
points <- data.frame()

par(mfrow=c(2,2))
for(cc in cn){
  
  tmp <- fdt[fdt$Country==cc, ]
  points <- tmp[, c("Total.Arrivals", "fc.fixed.effect", 'fc.OLS')]
  
grid.frame(x=as.numeric(tmp$Date), y=points)
  matplot(x=as.numeric(tmp$Date), y=points, lwd=1, lty=1, cex=1.25, pch=20, xlab="Time", ylab="Arrivals, thousands", add = T, type="o",
        col=c('salmon1', "firebrick1", 'cornflowerblue'))
  legendary2(c("Original", "Fixed effect fit", "OLS fit"), col=c('salmon1', "firebrick1", 'cornflowerblue'))
  mtext(cc, col="blueviolet", line=2, cex=1.25, adj = 0)
}

```

\newpage

\subsection{Statistics for accuracy}

We can measure the goodness of fit using several statistics. We will use the the \textit{mean absolute error} (MAE), \textit{mean squared error} (MSE)
and the \textit{mean absolute percentage error} (MAPE) [see @accuracy]. These statistics are defined by:

\[
MSE = \dfrac{1}{n}  \sum_{i=1}^{n}\left( y - \widehat{y} \right)^{2}
\]

\[
MAE = \dfrac{1}{n}  \sum_{i=1}^{n}\left| y - \widehat{y} \right|
\]

\[
MAPE = \dfrac{100}{n}  \sum_{i=1}^{n}\left| \dfrac{y - \widehat{y}}{y} \right| 
\]

Here

$\widehat{y}$ - fitted value

$y$ - original value


$n$ - number of observations


\begin{center}

```{r forecast accuracy, echo=F, results= "asis", fig.pos="H"}
  
fdt <- pdata[, c("Country", "Date", "Total.Arrivals", "fc.fixed.effect", 'fc.OLS')]
# fdt$Total.Arrivals  <- exp(fdt$Total.Arrivals)
# fdt$fc.fixed.effect <- exp(fdt$fc.fixed.effect)
# fdt$fc.OLS          <- exp(fdt$fc.OLS)

accuracy.table <- matrix(ncol=2, nrow=3) %>% as.data.frame()
colnames(accuracy.table) <- c("OLS", "Fixed effects")
rownames(accuracy.table) <- c("MSE", "MAPE", "MAE")

## MSE:

accuracy.table[1, 1] <- MSE(fdt$Total.Arrivals, fdt$fc.OLS)
accuracy.table[1, 2] <- MSE(fdt$Total.Arrivals, fdt$fc.fixed.effect)

## MAPE

accuracy.table[2, 1] <- MAPE(fdt$Total.Arrivals, fdt$fc.OLS)
accuracy.table[2, 2] <- MAPE(fdt$Total.Arrivals, fdt$fc.fixed.effect)

## MAE

accuracy.table[3, 1] <- MAE(fdt$Total.Arrivals, fdt$fc.OLS)
accuracy.table[3, 2] <- MAE(fdt$Total.Arrivals, fdt$fc.fixed.effect)

# options("scipen" = 5)

accuracy.table <- apply(accuracy.table, c(1, 2), function(x) {
  # x <- format(x,scientific=FALSE)
  x <- as.numeric(x)
  x <- round(x, 3)
  return(x)
  })


kable(accuracy.table[c(1,3), ], format = "latex", row.names = T, caption = "Table for accuracy")
```

\end{center}

As we can see from table 3 and the statistics for accuracy table the fixed effects models' predictions are closer to the original values. 

\newpage

\subsection{Random effects model}

There are two main types of panel models: the fixed effects model and the random effects models [see @econometric.methods, pp. 391 - 393]. The main difference between them is the assumption of $\alpha_{i}$ regarding the regressor matrix $X_{it}$. In the random effects model case it is assumed that $\alpha_{i}$ and $X_{it}$ do not correlate.

```{r random effects model, include=FALSE, echo=F}

model.random <- plm(formula = Total.Arrivals ~ Terror.attacks + GDP.per.capita , data=pdata, model="random")
# summary(model.random)
```
```{r, results='asis', echo=F, warning=F}
  # texreg(model.random, caption = "Random effects model", custom.coef.names = c("intercept", "attacks", "log GDP", "log Hours"), digits = 3)

tab <- xtable(summary(model.random)$coef, digits=c(3, 3, 3, 3, 3), caption = "Random effects model")
print(tab, type="latex", floating = T, only.contents = F, comment = F)
```

Comparing tables 2 and 4 we can see that the signs near the coefficients are the same and the actual values of the estimators are almost even. The random effects model has a little higher R statistic. The main difference here is that there is the intercept term and we lose some "uniqueness" to the countries which was present with the fixed effects model. One measure to determine which model is better for the given data is the Hausman test. 

```{r pHtest, echo=T, comment=NA}
  phtest(model.random, model.fixed)
w```

As we can see from the test result we cannot reject the null hypothesis therefore we conclude that the random effects coefficients are more efficient and are consistent. Therefore the test implies that we sould use the random effects model.

\newpage

\subsection{Residuals}

As we saw in the previous section the Hausman test for panel data indicated that we should use the random effects model. The test is only one of the criteria for deciding which model to use. Two critical assumptions of any linear model, including linear fixed-effects panel models and the random effects models, are homoscedastic and normally distributed errors.

```{r residuals of models, echo=F, fig.cap="Residuals of models", fig.height=7, fig.width=8,  fig.pos="H"}
  
par(mfrow=c(1,2))

res.random <- residuals(model.random)
res.fixed  <- residuals(model.fixed)
  
grid.frame(x=seq(from=1, length.out = length(res.random)), y=as.numeric(res.random))
matplot(x=seq(from=1, length.out = length(res.random)), y=as.numeric(res.random), lwd=1, lty=1, cex=1.25, pch=20, ylab="Residual value", xlab="Index", type="o", add=T, col=c('salmon1'))
mtext("Random effects model residuals", col="salmon1", line=1, cex=1.25, adj = 0)

grid.frame(x=seq(from=1, length.out = length(res.random)), y=as.numeric(res.random))
matplot(x=seq(from=1, length.out = length(res.fixed)), y=as.numeric(res.fixed), lwd=1, lty=1, cex=1.25, pch=20, ylab="Residual value", xlab="Index", type="o", col=c('cornflowerblue'), add=T)
mtext("Fixed effects model residuals", col="cornflowerblue", line=1, cex=1.25, adj = 0)

```

As we can see in figure 11 the residuals visually look very similar. 

```{r distribution of residuals, echo=F, fig.cap="Distribution of the residuals", fig.height=7, fig.width=8,  fig.pos="H"}
  
par(mfrow=c(2,2))

res.random <- residuals(model.random)
res.fixed  <- residuals(model.fixed)
  
hist(res.random, col = "cornsilk3", xlab = "Residuals of the random model", main="Density of the residuals")
hist(res.fixed, col = "cornsilk3", xlab = "Residuals of the fixed effects model", main="Density of the residuals")

qqnorm(res.random, ylab='Residuals', main = "Q - Q plot for random effects model", col="cornflowerblue")
grid()
qqline(res.random)

qqnorm(res.fixed, ylab='Residuals', main = "Q - Q plot for fixed - effects model", col="cornflowerblue")
grid()
qqline(res.fixed)

```

Figure 12 shows the histogram and the quantile to quantile plot of the residuals. In both cases, the residuals appear to follow a bell - shaped curve which implies normality. Altough the q q plots show that the residuals have heavy tails. This can be expected because real life data very often is not 'clean' enough to give normally distributed errors. A more important task is to test if the errors have a constant variance. 

```{r homoskedasticity, echo=F, fig.cap="Fits vs residuals", fig.height=7, fig.width=8,  fig.pos="H"}

par(mfrow=c(1,2))
res.fixed <- pdata$Total.Arrivals - pdata$fc.fixed.effect  
plot(pdata$fc.fixed.effect, res.fixed, col="deepskyblue4", xlab="Fixed effect model fits", ylab="Residuals of fixed effects model")

  pdata <- ddply(pdata, ~Country + Date, function(xframe){
    
    xframe <<- xframe
    
    ## random effects
  
    X <- xframe[, c('Terror.attacks', "GDP.per.capita")]
    coef <- model.random$coefficients
    fit <- coef[1] + coef[2] * X[1] + coef[3]* X[2]
    fit <- as.numeric(fit) # this is the fit of the logged number of arrivals
    xframe$fc.random.effect <- fit
    
    return(xframe)
  })

res.random <- pdata$Total.Arrivals - pdata$fc.random.effect 
plot(pdata$fc.random.effect, res.random, col="deepskyblue4", xlab="Random effect model fits", ylab="Residuals of Random effects model")

```

If there is no heteroskedasticity in the residuals we should see a pattern where the spread of points is uniform in the y-direction as we move along the x-axis. Figure 13 shows that both the models suffer from heteroskedasticity.

\subsection{Accuracy of fits}

```{r accuracy statistics of the models,  echo=F, comment=NA}
  
fdt <- pdata[, c("Country", "Date", "Total.Arrivals", "fc.fixed.effect", 'fc.random.effect')]
# fdt$Total.Arrivals    <- exp(fdt$Total.Arrivals)
# fdt$fc.fixed.effect   <- exp(fdt$fc.fixed.effect)
# fdt$fc.random.effect  <- exp(fdt$fc.random.effect)

accuracy.table <- matrix(ncol=2, nrow=3) %>% as.data.frame()
colnames(accuracy.table) <- c("Random effects", "Fixed effects")
rownames(accuracy.table) <- c("MSE", "MAPE", "MAE")

## MSE:

accuracy.table[1, 1] <- MSE(fdt$Total.Arrivals, fdt$fc.random.effect)
accuracy.table[1, 2] <- MSE(fdt$Total.Arrivals, fdt$fc.fixed.effect)

## MAPE

accuracy.table[2, 1] <- MAPE(fdt$Total.Arrivals, fdt$fc.random.effect)
accuracy.table[2, 2] <- MAPE(fdt$Total.Arrivals, fdt$fc.fixed.effect)

## MAE

accuracy.table[3, 1] <- MAE(fdt$Total.Arrivals, fdt$fc.random.effect)
accuracy.table[3, 2] <- MAE(fdt$Total.Arrivals, fdt$fc.fixed.effect)

# options("scipen" = 5)

accuracy.table <- apply(accuracy.table, c(1, 2), function(x) {
  # x <- format(x,scientific=FALSE)
  x <- as.numeric(x)
  x <- round(x, 3)
  return(x)
  })


kable(accuracy.table[c(1, 3), ], format = "latex", row.names = T, caption = "Random vs fixed model accuracy")


```

```{r visualizing the differences between random and fixed fits, echo=F, fig.cap="OLS and fixed effects model comparison", fig.height=7, fig.width=8,  fig.pos="H"}
  
fdt <- pdata[, c("Country", "Date", "Total.Arrivals", "fc.fixed.effect", 'fc.random.effect')]
# fdt$Total.Arrivals  <- exp(fdt$Total.Arrivals)/1000
# fdt$fc.fixed.effect <- exp(fdt$fc.fixed.effect)/1000
# fdt$fc.random.effect         <- exp(fdt$fc.random.effect)/1000
fdt$Date <- fdt$Date %>% as.character() %>% as.numeric()

## we will plot 4 countries in order to have a clear picture
cn <- c( "Australia",  "Austria", "France", "Lithuania")
points <- data.frame()

par(mfrow=c(2,2))
for(cc in cn){
  
  tmp <- fdt[fdt$Country==cc, ]
  points <- tmp[, c("Total.Arrivals", "fc.fixed.effect", 'fc.random.effect')]
  
grid.frame(x=as.numeric(tmp$Date), y=points)
  matplot(x=as.numeric(tmp$Date), y=points, lwd=1, lty=1, cex=1.25, pch=20, xlab="Time", ylab="Arrivals, thousands", add = T, type="o",
        col=c('salmon1', "firebrick1", 'cornflowerblue'))
  legendary2(c("Original", "Fixed effect fit", "random effect fit"), col=c('salmon1', "firebrick1", 'cornflowerblue'))
  mtext(cc, col="blueviolet", line=2, cex=1.25, adj = 0)
}

```

Inspecting the accuracy of the forecasts (figure 14 and table 5) we can see that the fixed effects model has an advantage. This is because the intercept for each of the countries is different in the fixed effects model thus giving us a more precise model for each individual country. Nevertheless, the Hausman test concluded that the random effects model is better than the fixed-effects one. 

\newpage

\subsection{Cross-validation}

In this section we will split our data into \textit{test} and \textit{training} parts and rerun the same fixed effects model. The goal of this analysis is to asses how the results of a statistical analysis will generalize to an independent data set. The test data will be up to year 2011 and the training set will be data from 2012 to 2014. 

```{r out of sample, echo=F, fig.pos="H"}

test.data <- pdata[pdata$Date %in% paste(1995:2011), ]  
training.data <- pdata[pdata$Date %in% paste(2012:2014), ]

model.train <- plm(formula = Total.Arrivals ~ Terror.attacks  + GDP.per.capita, data=test.data, model="random")

```

```{r, results='asis', echo=F, warning=F, fig.pos="H"}
  # texreg(model.train, caption = "Fixed effects model with training set", custom.coef.names = c("attacks", "log GDP", "log Hours"), digits = 3)

tab <- xtable(summary(model.train)$coef, digits=c(3, 3, 3, 3, 3), caption = "Fixed effects model")
print(tab, type="latex", floating = T, only.contents = F, comment = F)
```

Comparing the second and the sixth tables we can see that the sign of the coefficients are the same. The visual comparison of the out of sample analysis is in figure 10.


```{r out of sample predictions, echo=F, fig.pos="H",  fig.cap="Out of sample comparison", fig.height=7, fig.width=8,  fig.pos="H"}

  pdata <- ddply(pdata, ~Country + Date, function(xframe){
    
    xframe <<- xframe
    
    ## fixed effects
    
    X <- xframe[, c('Terror.attacks', "GDP.per.capita")]
    fit <- model.train$coefficients[1] + model.train$coefficients[2] * X[1] + model.train$coefficients[3] * X[2]
    fit <- as.numeric(fit) # this is the fit of the logged number of arrivals
    xframe$fc.w.test.data <- fit
    
    return(xframe)
  })



fdt <- pdata[, c("Country", "Date", "Total.Arrivals", "fc.w.test.data")]
# fdt$Total.Arrivals  <- exp(fdt$Total.Arrivals)/1000
# fdt$fc.w.test.data <- exp(fdt$fc.w.test.data)/1000
fdt$Date <- fdt$Date %>% as.character() %>% as.numeric()

## we will plot 4 countries in order to have a clear picture

cn <- c( "Australia",  "Austria", "France", "Lithuania")
points <- data.frame()

par(mfrow=c(2,2))
for(cc in cn){
  
  tmp <- fdt[fdt$Country==cc, ]
  points <- tmp[, c("Total.Arrivals", "fc.w.test.data")]
  
grid.frame(x=as.numeric(tmp$Date), y=points)
  matplot(x=as.numeric(tmp$Date), y=points, lwd=1, lty=1, cex=1.25, pch=20, xlab="Time", ylab="Arrivals, thousands", add = T, type="o",
        col=c("firebrick1", 'cornflowerblue'))
  legendary2(c("Original", "Test data model forecasts"), col=c("firebrick1", 'cornflowerblue'))
  mtext(cc, col="blueviolet", line=2, cex=1.25, adj = 0)
  abline(v=2012, lwd=2, lty=2, col="coral3")
}

# ggdata <- melt(test.data[, c("Country", "Date", "fc.random.effect", "fc.OLS")])
# ggdata <- ggdata[ggdata$Country=="Australia", ]
# p <- ggplot(ggdata, aes(x=Date, y=value, colour=variable, group=variable)) + geom_line()
# plot(p)

```

\newpage

\subsection{Dynamic panel data model}


In this section we will be creating a dynamic panel model (we will include lags of the variables). First of all, the estimation of the coefficient $\delta$ in the equation:

$$ y_{it} = \delta y_{it - 1} + \beta X_{it} + \alpha_{i} + \epsilon_{it} $$

is not as straightforward as just adding another collumn to the regressor matrix with the lagged variable $y_{it}$. This is because the OLS estimate will be biased. This is because the $y_{it}$ is a function $\epsilon_{it}$ and thus so is $y_{it-1}$ giving us the multicolinearity problem. 

Consider the simple model:

$$y_{it} = \delta y_{it - 1}+ \alpha_{i} + \epsilon_{it}$$

Taking the first differences we get rid of the individual effecct $\alpha_{i}$:

$$\Delta y_{it} = y_{it} - y_{it - 1} = \delta y_{it-1} + \alpha_{i} + \epsilon_{it} - \delta y_{it - 2} - \alpha_{i} - \epsilon_{it-1} = \delta (y_{it-1} - y_{it-2}) + \epsilon_{it} - \epsilon_{it-1} $$
The classic method to deal with multicolinearity is to find an instrumental variable that correlates with the endogenous variable and not with the residuals. Starting from time period t = 3:

$$\Delta y_{i3} = y_{i3} - y_{i2} = \delta (y_{i2} - y_{i1}) + (\epsilon_{i3} - \epsilon_{it-2}) $$
Here we can see that the variable $y_{i1}$ does not correlate with $(\epsilon_{i3} - \epsilon_{i2})$. 

When t = 4:

$$\Delta y_{i4} = y_{i4} - y_{i3} = \delta (y_{i3} - y_{i2}) + (\epsilon_{i4} - \epsilon_{i3}) $$
Both the $y_{i1}$ and ${y_{i2}}$ are good options. Going forward up to time period T we have the instrument vector:

$$(y_{i1}, y_{i2}, ..., y_{iT- 2})$$. If there are strictly exogenous variables in our system $x_{it}$ that correlate with the endogenous variable and not with the residuals they should also be included in the weight matrix diagonals (for a detailed proof [see @dynamic.panel]):


$$W_{i} = \begin{pmatrix}
    [y_{i1}, x_{i1}, x_{i2}]       & 0 & \dots & 0 \\
    0       &  [y_{i1}, y_{i2} x_{i1}, x_{i2}, x_{i3}] & \dots & 0 \\
    0      & 0  & \dots & [y_{i1}, y_{i2}, ..., y_{iT-2}, x_{i1}, x_{i2}, x_{i3}, ..., x_{iT-1}]
\end{pmatrix} $$

Then the consistent Arriano-Bond estimator is given by:

$$\widehat{\delta} =  \left[ (\Delta y_{t - 1})^{t} W (\widehat{V}_{W})^{-1} W^{t} \Delta y_{t - 1}\right]^{-1} x \left[ (\Delta y_{t - 1})^{t} W (\widehat{V}_{W})^{-1} W^{t} \Delta y_{t - 1}\right] $$
$$ \widehat{V}_{W} = \sum_{i=1}^{n} W_{i}^{t} \Delta \epsilon_{i} \Delta \epsilon_{i}^{t}  W_{i}$$

```{r dynamic model, include=FALSE, echo=F}

model.dynamic <- suppressWarnings( pgmm(Total.Arrivals ~ Terror.attacks + GDP.per.capita,
                       lag.form=list(1, 0, 0), gmm.inst = ~Total.Arrivals + Terror.attacks + GDP.per.capita, 
                       lag.gmm = list(c(2,99)),
                       effect="individual",
                       data=pdata))
```
```{r, results='asis', echo=F, warning=F, fig.pos="H"}
  # texreg(model.train, caption = "Fixed effects model with training set", custom.coef.names = c("attacks", "log GDP", "log Hours"), digits = 3)

tab <- xtable(summary(model.dynamic)$coef, digits=c(3, 3, 3, 3, 3), caption = "Dynamic panel data model")
print(tab, type="latex", floating = T, only.contents = F, comment = F)
```

After trying various laggs and taking into account the economic logic behind the model we get the results in table 7. Note that the variable regarding the hours worked is dropped due to insignificance. We can see that the number of arrivals in the previous year has a big effect to the current year's number of arrivals. The \textit{plm} package does not have a working function to forecast a dynamic panel model thus a code which enables us to do so is included in the \textbf{Additional R code} section.

```{r obtaining the dynamic model fits, echo=F, fig.cap="Dynamic model's fits", fig.height=7, fig.width=8,  fig.pos="H"}

pdata <- ddply(pdata, ~Country, function(xframe){
  
  xframe <<- xframe
  
  xframe$fc.dynamic <- predict.pgmm.custom(model.dynamic, xframe$Total.Arrivals, xframe[, c("Terror.attacks", "GDP.per.capita")])
  
  return(xframe)
  
})


```

\subsection{Dynamic models' residuals}

```{r residuals of dynamic models, echo=F, fig.cap="Residuals of models", fig.height=10, fig.width=7,  fig.pos="H"}
  
par(mfrow=c(3, 1))

res.dynamic <- model.dynamic$residuals %>% unlist
  
grid.frame(x=seq(from=1, length.out = length(res.dynamic)), y=as.numeric(res.dynamic))
matplot(x=seq(from=1, length.out = length(res.dynamic)), y=as.numeric(res.dynamic), lwd=1, lty=1, cex=1.25, pch=20, ylab="Residual value", xlab="Index", type="o", add=T, col=c('salmon1'))
mtext("Dynamic model residuals", col="salmon1", line=1, cex=1.25, adj = 0)

hist(res.dynamic, col = "cornsilk3", xlab = "Residuals of the dynamic model", main="Density of the residuals")

qqnorm(res.fixed, ylab='Residuals', main = "Q - Q plot for dynamic model", col="cornflowerblue")
grid()
qqline(res.fixed)

```
As we can see in figure 17 the residuals resemble a simmilar pattern as in the case with static panel models. Altough the quantile to quantile plot shows "lighter" tails than in the previous cases. 


\newpage

\section{Tests}

In this paper there were implemented two tests in order to inspect the panel models: the F test for individual effects and the Haussman test to determine which model - random or fixed effects - is more suitable. 

\subsection{F test}

The f test [see @F.test, pp.3-4] that is used with the panel models in R statistical package tests the following hypothesis: assume that we have two models: 

restricted
$$y_{it} = \alpha + \beta X_{it} + \epsilon_{it}$$
and unrestricted:

$$y_{it} = \alpha + u_{i} + \beta X_{it} + \epsilon_{it}$$

Then the hypothesis is 


\[\begin{array}{lr}
        H_{0}: u_{i} =0,  \forall i \\
        H_{1}: \exists i:    u_{i} \neq 0,  
        \end{array}\]

The alternative hypothesis would mean that there exists a country where adding the term $u_{i}$ the r-squared statistic increases. 

To test this hypothes we use the F statistic (hence the name of the test):

$$F = \dfrac{(ESS_{R} - ESS_{U})/(N- 1) }{ESS_{U}/((T-1)N- K)}$$

Here 

$ESS_{U}$ - error sum of squares of the unrestricted model

$ESS_{R}$ - error sum of squares of the restricted model

$N$ - number of countries

$T$ - number of time periods

Under the assumption that the residuals are gaussian then the F statistic is distributed by the $\textbf{F}$ distribution with N-1, N(T-1) - K degrees of freedom.

\newpage

\subsection{Hausman test for panel models}

The Hausman test [see @Hausman.test, pp.39-40] is used to determine whether $X_{it}$ are uncorrelated with $\alpha_{i}$. To test this we denote two estimators:

$\widehat{\beta_{FE}}$ - fixed effects estimate. 

$\widehat{\beta_{RE}}$ - random effects estimate.

The hypothesis is then:

 Estimator                $H_{0}$ holds              $H_{0}$ is rejected  
-------                  -------------              -----------------
$\widehat{\beta_{RE}}$    consistent;                 Inconsistent
                          efficient
$\widehat{\beta_{FE}}$    consistent;                  consistent       
                          inefficient

-------                  -------------              -----------------


Since under null hypothesis $\widehat{\beta_{RE}}$ is efficient then 

$$Var(\widehat{\beta_{FE}} - \widehat{\beta_{RE}}) =Var(\widehat{\beta_{FE}}) - Var(\widehat{\beta_{RE}})$$
Then the Hausman statistic is defined as:

$$H = \left( \widehat{\beta_{FE}} - \widehat{\beta_{RE}} \right) ^{T} \left(Var(\widehat{\beta_{FE}}) - Var(\widehat{\beta_{RE}})\right) \left( \widehat{\beta_{FE}} - \widehat{\beta_{RE}} \right)$$

and 

$$H \overset{asy}{\sim} \chi^2$$

The main idea of this test is that if $Var(\widehat{\beta_{FE}})$ is smaller than  $Var(\widehat{\beta_{RE}})$ it means that the H statistic will be "large" and the null hypothesis will be rejected. This in turn means that $\widehat{\beta_{RE}}$ is no efficient because there exists another statistic with a smaller variance.

\newpage

\section{Conclusion}

After testing various models a conclusion was met that the fixed effects panel data model best suits the data regarding the arrivals to the OECD countries. According to the model (the model is in table 2):

$\bullet$ Every terrorist attack shrinks the total number of arrivals on average by 0.5 percent in a given year.

$\bullet$ Increasing the total GDP per capita of a given country increases the tourist flows by 0.493 percent.

$\bullet$ A decrease of a 1 percent of total average working hours of neighbouring countries would increase the tourism flow by 0.8 percent.

\newpage

# References



